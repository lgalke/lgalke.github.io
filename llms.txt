# Lukas Galke Poech

> Tracing connectionist mechanisms for safer AI systems.

Assistant Professor,  University of Southern Denmark (SDU),  Department of Mathematics and Computer Science (IMADA),  Data Science Section,  Center for Machine Learning

**Mission:** In my view, understanding why the most powerful AI systems behave the way they do is the most important scientific problem in AI safety. I advance interpretability methods to make AI systems more transparent in order to inform alignment and enable principled control.

**Website:** https://lgalke.github.io


---
title: CV
---

## General Information

**Name**: Lukas Paul Achatius Galke Poech

**Address**: Campusvej 55, 5230 Odense, Denmark

**Office**: Ø12-509b-2

## Experience

Current position (since 2024): **Assistant Professor**, University of Southern Denmark (SDU), Department of Mathematics and Computer Science (IMADA), Section for Data Science and Statistics (DSS), Centre for Machine Learning (C4ML)

2022-2024: **Postdoctoral Researcher**, Max Planck Institute for Psycholinguistics

2017-2022: **Doctoral Researcher**, Kiel University & ZBW

## Education

2022: PhD in Computer Science, Kiel University, Germany

2017: M.Sc. in Computer Science, Kiel University, Germany

2013: BSc in Computer Science, Kiel University, Germany

## Grants and Major Projects

**MIST: Scalable Mechanistic Interpretability for Safe and Trustworthy LLM Agents** (2026–2031)
Novo Nordisk Foundation, Principal Investigator,
Funding for 2 PhD + 1 Postdoc position to develop interpretability methods for understanding and controlling LLM agents.

**Sustainable Language Modeling through Quantization-Aware Continual Pre-training** (2025)
EuroHPC JU AI and Data Intensive Applications, Principal Investigator,
200,000 GPU hours

**Danish Foundation Models** (2025–2029)
Danish Government Initiative, Work Package Co-Lead (Evaluation), co-supervising 5 PhD students

## Selected publications

- Mogens From, Jacob Nielsen, Lukas Galke, and Peter Schneider-Kamp (2026). DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes. *AAAI*.
- Danial Namazifard and Lukas Galke (2025). Isolating Culture Neurons in Multilingual Large Language Models. To appear in: *AACL-IJCNLP Findings*.
- Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, and Lukas Galke (2025). Guarded Query Routing for Large Language Models. *ECAI*.
- Jacob Nielsen, Peter Schneider-Kamp, and Lukas Galke (2025). Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models? *ACL Findings*.
- Lukas Galke, Yoav Ram, and Limor Raviv (2024) Deep neural networks and humans both benefit from compositional language structure. Nature Communications 15:10816.

## Selected invited talks

- Evaluating Large and Multilingual Language Models through Citizen Science. AI, Citizen Science, and MedTech – An Exploration, 2025, May 13, SDU, Odense, Denmark.
- Emergent communication and learning pressures in language models. Workshop on Using Artificial Neural Networks for Studying Human Language Learning and Processing, 2024, June 10, University of Amsterdam.
- What makes a language easy to deep-learn? Computational Linguistics Seminar, 2023, May 16, University of Amsterdam, Netherlands.

## Teaching experience

- AI509: **Natural Language Processing** -- Fall 2025, SDU (main lecturer)
- AI508: **Computer Vision** -- Fall 2025, SDU (co-lecturer)
- DSK809: **Deep Learning** -- Fall 2025, SDU (responsible)
- AI506: **Advanced Machine Learning** -- Spring 2025, SDU (main lecturer)
- DM873/DS809: **Deep Learning** -- Fall 2024, SDU (main lecturer)

## Academic service

- **Program Chair**: ICNLSP 2025
- **Program Committee**: ACL, EMNLP, ICLR, ICML, AAAI, ECAI, ...
- **Journal Reviewer**: Nature Human Behavior, Nature Communications, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Knowledge and Data Engineering, Neural Networks, Pattern Recognition, Journal of Artificial Intelligence Research (JAIR), ...


---
title: Projects
---

## MIST: Scalable Mechanistic Interpretability for Safe and Trustworthy LLM Agents (starting 2026) {#mist}

The MIST project, funded by the Novo Nordisk Foundation, addresses the critical challenge of understanding and controlling large language model (LLM) agents. We develop scalable, theory-driven methods for mechanistic interpretability to identify the underlying mechanisms responsible for tool use, reasoning, and multi-agent communication. By investigating the cross-model and cross-lingual universality of these mechanisms, we create interpretability-informed steering techniques and latent guardrails that enable safe deployment in high-stakes applications. The project ultimately aims to establish a comprehensive framework for issuing safety certificates to LLM agents through systematic evaluation in diverse multi-agent simulation environments.

## Danish Foundation Models (since 2025) {#danish-foundation-models}

The aim of the Danish Foundation Models project is to develop and evaluate large multilingual and multimodal foundation models that excel at Danish -- trained only on permissible Danish data and providing access to the full models (open-weight) and their source code (open-source). The project is a collaborative effort between the University of Southern Denmark, Aarhus University, Copenhagen University, and the Alexandra Institute.

- [Project website](https://foundationmodels.dk)

## Interpretability of Language Models (since 2023) {#interpretability}

Large language models have shown incredible results and experienced a rapid
widespread adoption. However, we do not know *how* the trained models come to
their decisions of what text to generate. Therefore, this project aims to
investigate the interpretability of language models through structural probing,
behavioral probing, and mechanistic interpretability -- and by bringing theories
and experimental paradigms from psycholinguistics into machine learning.

- Paper: [**Deep neural networks and humans both benefit from compositional language structure**](https://rdcu.be/d5f2e). *Nature Communications* 15:10816. 
- Paper: [Learning and communication pressures in neural networks: Lessons from emergent communication](https://doi.org/10.34842/3vr5-5r49). *Language Development Research* 5(1).  
- Paper: [Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test](https://aclanthology.org/2024.cmcl-1.15/) published in the [CMCL workshop](https://cmclorg.github.io) at the [ACL 2024 conference](https://2024.aclweb.org).
- Poster at the [Highlights in the Language Sciences conference](https://hils2024.nl), July 8--11, 2024
- Poster at the workshop on [Using Artificial Neural Networks for Studying Human Language Learning and Processing](https://ann-humlang.github.io), June 10--12, 2024
- Extended abstract on Testing the Linguistic Niche Hypothesis in Large Language Models with a Multilingual Wug Test published in the [Proceedings of Evolang XV](https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3587960) (pp. 91--94)
- Podium presentation by Anh Dang at [Evolang 2024](https://evolang2024.github.io)
- Talk at the [Protolang 8 conference](https://sites.google.com/view/protolang8/home), Rome, September 27--28, 2023. Abstract published in the [Protolang 8 book of abstracts](https://drive.google.com/file/d/1rbBGo82zV4nCNgRiLeGXEJoR79wSGxpu/view)

## Machine Communication and the Emergence of Language (2022--2024) {#mc-emecom}

Imagine you put artificial neural network agents into a box and give them a task
that can only be solved with communication. How do neural network agents learn
to communicate? What communication protocols emerge? What are the parallels to
human communication?

- Machine Communication and the Emergence of Language. Talk given at the MPI Proudly Presents series, July 4, 2024, [Max Planck Institute for Psycholinguistics](https://www.mpi.nl), Nijmegen, NL.
- Invited talk at the workshop on [Using Artificial Neural Networks for Studying Human Language Learning and Processing](https://ann-humlang.github.io), June 10--12, 2024
- [Machine Learning and the Evolution of Language workshop at the Joint Conference on Language Evolution](https://ml4evolang.github.io)
- [Emergent Communication workshop paper at ICLR](https://openreview.net/forum?id=rqUGZQ-0XZ5)

## What Matters for Text Classification (2021--2024) {#textclf}

Automatically categorizing text documents is a popular research topic with
numerous new approaches being published each year. However, do they really give
an advantage over earlier approaches? This question has triggered this line of
research and the answer we have is worrisome. For instance, many recently
proposed methods such as TextGCN are outperformed by a simple multilayer
perceptron on a bag-of-words representation -- a decade old technique enhanced
with today's best practices.

Even in the era of language models, the results on text classification are sometimes counter-intuitive.
As such, fine-tuned small models typically outperform large language models.

- [Simplifying hierarchical text classifciation](https://ebooks.iospress.nl/doi/10.3233/FAIA240661)
- [preprint of a comparative survey paper](https://arxiv.org/abs/2204.03954)
- [ACL 2022 paper](https://doi.org/10.18653/v1/2022.acl-long.279)
- [code for multi-label classification](https://github.com/drndr/multilabel-text-clf)
- [code for single-label classification](https://github.com/lgalke/text-clf-baselines)
- [extra code for single-label classification with different methods](https://github.com/FKarl/text-classification)

## Lifelong Graph Representation Learning (2019--2024) {#lifelong-graph-learning}

Graph neural networks have quickly risen to be the standard technique for machine learning on graph-structured data.  Yet graph neural networks are usually only applied to static snapshots of graphs, while real-world graphs (social media, publication networks) are continually evolving.  Evolving graphs come with challenges that are rarely reflected in the graph representation learning literature, such as dealing with new classes in node classification.  We pursue a lifelong learning approach for graph neural networks on evolving graphs and investigate incremental training, out-of-distribution detection, and issues caused by an imbalanced class distribution.

- [CoLLAs 2024 conference paper on zero-shot learning in graphs](https://lifelong-ml.cc/Conferences/2024/acceptedpapersandvideos/conf-2024-38) or on [arXiv](https://arxiv.org/abs/2406.09926)
- [Code for CoLLAs 2024 paper](https://github.com/Bobowner/POWN)
- [IJCNN 2023 conference paper on new class detection in graphs](https://doi.org/10.1109/IJCNN54540.2023.10191071)
- [*Neural Networks* journal paper, 2023](https://pure.mpg.de/rest/items/item_3368482_4/component/file_3510107/content)
- [IJCNN 2021 conference paper](https://doi.org/10.1145/3197026.3197050)
- [ICLR 2019 workshop paper](https://rlgm.github.io/papers/21.pdf)
- [Code for 2023 *Neural Networks* paper](https://github.com/lgalke/lifelong-learning)
- [Code for IJCNN 2023 paper](https://github.com/Bobowner/Open-World-LGL)
- [Code for ICLR 2019 workshop paper](https://github.com/lgalke/gnn-pretraining-evaluation)
- related project: [Q-AKTIV](#q-aktiv)

## Analyzing the Scientific, Economic and Societal Impact of Research Activities and Research Networks (Q-AKTIV) (2019--2022, funded by BMBF) {#q-aktiv}

The aim of Q-AKTIV is to improve the methods for forecasting dynamics and interactions between research, technology development, and innovation. The network analysis methods will be based on recent developments in Deep Learning. In addition to the emergence of new knowledge areas and networks, we focus on the convergence processes of established sectors. The development and evaluation of the new methods initially takes place in the field of life sciences, which is characterized by high marked dynamics. The additional application in economics enables a systematic comparison of the dynamics between the disciplines of science. The new methods will be used to predict the impact of existing research and network structures on the dynamics of knowledge and technologies as well as the future relevance of topics and actors. The result of Q-AKTIV is an implemented and evaluated instrument for the strategic analysis and prognosis of the dynamics in science and innovation. This complements today’s primarily qualitative approaches to early strategic planning and increases the decision-making ability of research institutions, policy makers, and industries. In addition to the analysis of dynamics, also valuable indicators for R & D performance measurement can be derived, e.g., the registration of patents based on scientific publications, the economic development of the companies involved, as well as the outreach of research activities. The practice partner brings in the necessary experience in the field of business valuation and strategy development and ensures a practical testing of the toolkit.

- [Scientometrics journal paper on the lack of interdisciplinarity in the scientific response to COVID-19, 2024](https://doi.org/10.1007/s11192-024-05132-x)
- [TEM journal paper evaluating the developed methods, 2023](https://doi.org/10.1109/TEM.2023.3308008)
- [project website](https://q-aktiv.github.io)
- [COVID-19++ dataset](https://doi.org/10.5281/zenodo.5531084)
- [COVID-19++ dataset paper](https://doi.org/10.1109/BigData52589.2021.9671730).
- [conference paper on learning concept representations](https://doi.org/10.18420/inf2019_26)
- [follow-up work building on concept representation learning methods](https://doi.org/10.1111/jpim.12572)
- [Python package for learning concept representations and analyzing network dynamics](https://gitlab.com/Q-Aktiv/qgraph)
- [workshop paper on data enrichment 2](https://ceur-ws.org/Vol-2773/paper-09.pdf)
- [workshop paper on data enrichment 1](https://ceur-ws.org/Vol-2451/paper-23.pdf)
- [tools for harvesting raw data](https://github.com/Q-AKTIV/covid19-harvesting-tools)
- [interview about open science tools for digital collaboration (en)](https://www.zbw-mediatalk.eu/en/2019/03/praxisbericht-open-science-diese-tools-foerdern-die-zusammenarbeit/)
- [interview about open science tools for digital collaboration (de)](https://www.zbw-mediatalk.eu/de/2019/03/praxisbericht-open-science-diese-tools-foerdern-die-zusammenarbeit/)
- [journal paper (German)](https://www.universitaetsverlagwebler.de/_files/ugd/7bac3c_24fe9adc2e3740178ad5ba98f66d1931.pdf)
- [press item (German)](https://www.b-i-t-online.de/neues/5386)
- [press release (ZB MED, German)](https://www.zbmed.de/forschen/abgeschlossene-projekte/q-aktiv)
- [funding announcement (German)](https://www.wihoforschung.de/wihoforschung/de/bmbf-projektfoerderung/foerderlinien/quantitative-wissenschaftsforschung/q-aktiv/q-aktiv_node.html)
- [final project report (German)](https://zenodo.org/records/5788648)

## Representation Learning for Texts and Graphs (2017--2022) {#phd-project}

My PhD project. A meta-project bringing together [word2mat](#word2mat),
[textclf](#textclf), [aaerec](#aaerec), [lifelong-graph-learning](#lifelong-graph-learning). [phd
thesis](https://doi.org/10.21941/kcss/2023/1)

## Word Matrices for Text Representation Learning (word2mat) (2017-2022) {#word2mat}

The idea of this project was to embed each word as a matrix as an alternative
to vectors used in word embeddings. By using matrix embeddings instead of
vector embeddings, we can use matrix multiplication as a composition function
to form a representation for phrases and sentences. While word matrices alone
did not exceed the performance of word vectors, a combination of word matrices
and word vectors turned out to be beneficial. Later, we showed that pre-trained
language models can be distilled into such a purely embedding-based model,
giving benefits in efficiency while keeping reasonable accuracy.

- [IJCNN paper on distilling the knowledge from a pre-trained language model into matrix embedding models](https://doi.org/10.1109/IJCNN55064.2022.9892144)
- [extended abstract in the best-of data science track at the INFORMATIK 2019](https://doi.org/10.18420/inf2019_47)
- [ICLR paper on self-supervised learning of word matrices](https://openreview.net/pdf?id=H1MgjoR9tQ)
- [code](https://github.com/florianmai/word2mat)

## Autoencoders for Document-based Recommendations (aaerec) (2017--2022) {#aaerec}

The aim of this project was to build a document-level citation recommendation
system that could, for example, make users aware of missing references.  A
specialty of this project compared to other recommender systems is that we do
not use any a user data or a user profile but only operate on the contents of
the current draft. The main research question was whether models could be
enhanced by using textual side information, such as the title of the draft,
which we confirmed for a wide range of autoencoder-based recommendation models.
Interestingly, we found that the choice of the best model depends on the
semantics of item co-occurrence.  When item co-occurrence implies relatedness
(as in citations), looking at other items is far more useful than looking at
the text. In contrast, when item co-occurrence implies diversity, such as in
subject labels from professional subject indexers, the text is more useful.

- [journal paper on citation and subject label recommendation](https://doi.org/10.1007/s10791-022-09408-9)
- [conference paper on citation and subject label recommendation](https://dl.acm.org/doi/abs/10.1145/3209219.3209236)
- [RecSys challenge workshop paper on music recommendation for automatic playlist continuation](https://doi.org/10.1145/3267471.3267476)
- [codebase for all three papers](https://github.com/lgalke/aae-recommender)

## Linked Open Citation Database (LOC-DB) (2017--2020, funded by DFG) {#loc-db}

The LOC-DB project will develop ready-to-use tools and processes based on the
linked-data technology that make it possible for a single library to
meaningfully contribute to an open, distributed infrastructure for the
cataloguing of citations. The project aims to prove that, by widely
automating cataloguing processes, it is possible to add a substantial benefit
to academic search tools by regularly capturing citation relations. These
data will be made available in the semantic web to make future reuse
possible. Moreover, we document effort, number and quality of the data in a
well-founded cost-benefit analysis.  The project will use well-known methods
of information extraction and adapt them to work for arbitrary layouts of
reference lists in electronic and print media. The obtained raw data will be
aligned and linked with existing metadata sources. Moreover, it will be shown
how these data can be integrated in library catalogues. The system will be
deployable to use productively by a single library, but in principle it will
also be scalable for using it in a network.

- [conference paper on citation recommendation](https://dl.acm.org/doi/abs/10.1145/3209219.3209236)
- [main paper on the project as a whole](https://dl.acm.org/doi/abs/10.1145/3197026.3197050)
- [demo of the LOC-DB project outcome](https://locdb.bib.uni-mannheim.de/demo-frontend/frontpage)
- [collection of code for the LOC-DB project](https://github.com/locdb)
- Second Linked Open Citation Database (LOC-DB) workshop, 2018, Mannheim, Germany.
- First Linked Open Citation Database (LOC-DB) workshop, 2017, Mannheim, Germany.
- [project website (de)](https://www.bib.uni-mannheim.de/ihre-ub/projekte-der-ub/linked-open-citation-database/)

## Word Embeddings for Information Retrieval (vec4ir) (2016--2017) {#vec4ir}

The key idea was to use word embeddings for similarity scoring in information
retrieval. The two main take-aways:

1. It is important to retain the crisp matching operation (before similarity scoring), even when using word embeddings.
2. A combination of classic information retrieval method TF-IDF and word embeddings led to the best results.

- [INFORMATIK 2017 paper](https://doi.org/10.18420/in2017_215)
- [MSc thesis](/pdf/MSc-thesis_LG.pdf)
- [codebase for studying embedded retrieval (>200 stars, >40 forks)](https://github.com/lgalke/vec4ir)

## TraininG towards a society of data-saVvy inforMation prOfessionals to enable open leadership INnovation (MOVING) (2016--2019, EU funding, Grant Agreement Number 693092)

I engaged in this EU Horizon 2020 project as a research assistant between 2016 and 2017, leading to contributions to the deliverables 3.1, 3.2 and 3.3, as well as various conference and workshop papers:

- [Deliverable 3.3](http://moving-project.eu/wp-content/uploads/2019/03/moving_d3.3_v1.0.pdf)
- [Deliverable 3.2](http://moving-project.eu/wp-content/uploads/2018/03/moving_d3.2_v1.0.pdf)
- [Deliverable 3.1](http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf)
- [ICADL 2018 paper on information retrieval on titles vs. full-text](https://doi.org/10.1007/978-3-030-04257-8_30)
- [DEXA 2018 workshop paper on response suggestion](https://doi.org/10.1007/978-3-319-99133-7_18)
- [code for response suggestion](https://github.com/lgalke/resuggest)
- [Project website](http://moving-project.eu)

## Extreme Multi-label Text Classification (Quadflor) (2015--2018) {#xlmc}

This project originated from my Master's project (2015--2016), where we
developed a pipeline for extreme (=many possible classes) multi-label
text classification.  We found that a multi-layer perceptron beats the
state-of-the-art kNN approach by more than 30%.  Moreover, we compared using
either the full-text or only the title of a research paper as a basis for
classification. The result was that the full-text is only marginally better
than the title. My team member Florian Mai investigated the trade-off between
full-text and title in his Master's thesis, finding that the increased
availability of title data compensates for increased information in full-text
articles.

- [code](https://github.com/quadflor/Quadflor) (bought by [ZBW](https://zbw.eu) for production usage)
- [K-CAP 2017 paper](https://doi.org/10.1145/3148011.3148039) (outcome of the Master's project)
- [JCDL 2018 paper](https://doi.org/10.1145/3197026.3197039) (outcome of Florian's Master's thesis)


---
title: Publications
---

## 2025–2026

- Mogens From, Jacob Nielsen, Lukas Galke, and Peter Schneider-Kamp (2026). DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes. To appear in: *AAAI*.
- Danial Namazifard and Lukas Galke (2025). Isolating Culture Neurons in Multilingual Large Language Models. To appear in: *AACL-IJCNLP Findings*. [preprint](https://arxiv.org/abs/2508.02241)
- Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, and Lukas Galke (2025). Guarded Query Routing for Large Language Models. *ECAI*. [preprint](https://arxiv.org/abs/2505.14524)
- Marcel Hoffmann, Lukas Galke, and Ansgar Scherp (2025). Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax. *ECAI*.
- Mourad Abbas, Tariq Yousef, and Lukas Galke (Eds.) (2025). Proceedings of the 8th International Conference on Natural Language and Speech Processing (ICNLSP-2025). [proceedings](https://aclanthology.org/2025.icnlsp-1.0/)
- Filippo Tonini and Lukas Galke (2025). Super-additive Cooperation in Language Model Agents. *Springer 3rd International Conference on Frontiers of Artificial Intelligence, Ethics, and Multidisciplinary Applications*.
- Stine Lyngso Beltoft and Lukas Galke (2025). Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI. *Springer 3rd International Conference on Frontiers of Artificial Intelligence, Ethics, and Multidisciplinary Applications*.
- Andrea Blasi Nuñez, Lukas Galke, and Peter Schneider-Kamp (2025). MLDataForge: Accelerating Large-Scale Dataset Preprocessing and Access for Multimodal Foundation Model Training. *RaNLP*.
- Jacob Nielsen, Peter Schneider-Kamp, and Lukas Galke (2025). Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models? *ACL Findings*. [preprint](https://arxiv.org/abs/2502.11895)
- Andor Diera, Lukas Galke, and Ansgar Scherp (2025). Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search. *ESANN*. [preprint](https://arxiv.org/abs/2411.17538) [code](https://github.com/drndr/code_isotropy)
- Jacob Nielsen, Lukas Galke, and Peter-Schneider Kamp (2025). When are 1.58 bits enough? A Bottom-up Exploration of Quantization-aware Training with Ternary Weights. *ICAART*. [preprint](https://arxiv.org/abs/2411.05882)
- Lukas Galke, Limor Raviv (2025). Learning and communication pressures in neural networks: Lessons from emergent communication. *Language Development Research* 5(1). [paper](https://doi.org/10.34842/3vr5-5r49) [preprint](https://arxiv.org/abs/2403.14427)
- Andor Diera, Lukas Galke, Fabian Karl, and Ansgar Scherp (2025). Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck. *ICNLSP*. [paper](https://aclanthology.org/2025.icnlsp-1.17/)
- Thao Anh Dang, Limor Raviv, and Lukas Galke (2025). Tokenization and Morphology in Multilingual Language Models: A Comparative Analysis of mT5 and ByT5. *ICNLSP*. [paper](https://aclanthology.org/2025.icnlsp-1.24/)

## 2024

- Lukas Galke, Yoav Ram, Limor Raviv (2024). Deep neural networks and humans both benefit from compositional language structure. *Nature Communications* 15:10816. [paper](https://rdcu.be/d5f2e) [code](https://github.com/lgalke/easy2deeplearn) [data](https://doi.org/10.5281/zenodo.14205452)
- Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (2024). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19. *Scientometrics*. [paper](https://doi.org/10.1007/s11192-024-05132-x)
- Yousef Younes, Lukas Galke, and Ansgar Scherp (2024). RADAr: A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification. *ECAI*. [paper](https://ebooks.iospress.nl/doi/10.3233/FAIA240661) [code](https://github.com/yousef-younes/RADAr)
- Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2024). POWN: Prototypical Open-world Node Classification. *CoLLAs*. [paper](https://lifelong-ml.cc/Conferences/2024/acceptedpapersandvideos/conf-2024-38) [code](https://github.com/Bobowner/POWN)
- Anh Dang, Limor Raviv, Lukas Galke (2024). Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test. *CMCL Workshop @ ACL*. [paper](https://aclanthology.org/2024.cmcl-1.15/)
- Lukas Galke, Yoav Ram, Limor Raviv (2024). Learning Pressures and Inductive Biases in Emergent Communication: Parallels between Humans and Deep Neural Networks. *Evolang XV*.
- Anh Dang, Limor Raviv, Lukas Galke (2024). Testing the Linguistic Niche Hypothesis in Large Language Models with a Multilingual Wug Test. *Evolang XV*.

## 2023

- Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad U. Förstner, Klaus Tochtermann, Carsten Schultz (2023). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence. *IEEE Transactions on Engineering Management*. [paper](https://doi.org/10.1109/TEM.2023.3308008)
- Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. *Neural Networks* 164, 156-176. [paper](https://pure.mpg.de/rest/items/item_3368482_4/component/file_3510107/content) [code](https://github.com/lgalke/lifelong-learning)
- Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2023). Open-World Lifelong Graph Learning. *IJCNN*. [paper](https://doi.org/10.1109/IJCNN54540.2023.10191071) [code](https://github.com/Bobowner/Open-World-LGL)
- Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp (2023). GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. *GenBench Workshop @ EMNLP*. [paper](https://aclanthology.org/2023.genbench-1.2/)
- Lukas Galke, Yoav Ram, Limor Raviv (2023). What makes a language easy to deep-learn? *Protolang 8*.
- Lukas Galke (2023). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability. Number 2023/1 in Kiel Computer Science Series. Department of Computer Science. Dissertation, Faculty of Engineering, Kiel University. [pdf](https://doi.org/10.21941/kcss/2023/1)

## 2022

- Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. *Inf Retrieval J* 25, 269–305. [paper](https://doi.org/10.1007/s10791-022-09408-9) [code](https://github.com/lgalke/aae-recommender)
- Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand Nölscher, Angelina Sonderecker, Ansgar Scherp (2022). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. *IJCNN*. [paper](https://doi.org/10.1109/IJCNN55064.2022.9892144)
- Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. *ACL*. [paper](https://doi.org/10.18653/v1/2022.acl-long.279) [code](https://github.com/lgalke/text-clf-baselines)
- Lukas Galke, Yoav Ram, Limor Raviv (2022). Emergent communication for understanding human language evolution: What's missing? *Emergent Communication Workshop @ ICLR*. [paper](https://openreview.net/forum?id=rqUGZQ-0XZ5)
- Lukas Galke (2022). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability [selected PhD thesis abstract]. *IEEE Intelligent Informatics Bulletin*, 22(1), 52. [complete issue](https://www.comp.hkbu.edu.hk/~cib/2022/IIB2022_Final.pdf)

## 2021

- Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement. *Hochschulmanagement* 16(1). [complete issue](https://www.universitaetsverlagwebler.de/_files/ugd/7bac3c_24fe9adc2e3740178ad5ba98f66d1931.pdf)
- Lukas Galke, Benedikt Franke, Tobias Zielke, Ansgar Scherp (2021). Lifelong Learning of Graph Neural Networks for Open-World Node Classification. *IJCNN*. [paper](https://doi.org/10.1109/IJCNN52387.2021.9533412) [code](https://github.com/lgalke/lifelong-learning)
- Lukas Galke, Eva Seidlmayer, Gavin Lüdemann, Lisa Langnickel, Tetyana Melnychuk, Konrad U Förstner, Klaus Tochtermann, Carsten Schultz (2021). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics. *IEEE Big Data*. [paper](https://doi.org/10.1109/BigData52589.2021.9671730)

## 2020 and earlier

- Eva Seidlmayer, Jakob Voß, Tatyana Melnychuk, Lukas Galke, Klaus Tochtermann, Carsten Schultz, Konrad U. Förstner (2020). ORCID for Wikidata — Data enrichment for scientometric applications. *Wikidata Workshop @ ISWC*. [paper](https://ceur-ws.org/Vol-2773/paper-09.pdf)
- Lukas Galke, Tetyana Melnychuk, Eva Seidlmayer, Steffen Trog, Konrad U. Förster, Carsten Schultz, Klaus Tochtermann (2019). Inductive Learning of Concept Representations from Library-Scale Bibliographic Corpora. *INFORMATIK*. [paper](https://doi.org/10.18420/inf2019_26)
- Florian Mai, Lukas Galke, Ansgar Scherp (2019). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model. *ICLR*. [paper](https://openreview.net/pdf?id=H1MgjoR9tQ) [code](https://github.com/florianmai/word2mat)
- Eva Seidlmayer, Lukas Galke, Tatyana Melnychuk, Carsten Schultz, Klaus Tochtermann, Konrad U. Förstner (2019). Take it Personally — A Python library for enrichment in informetrical applications. *Posters&Demos @ SEMANTICS*. [paper](https://ceur-ws.org/Vol-2451/paper-23.pdf)
- Lukas Galke, Iacopo Vagliano, Ansgar Scherp (2019). Can Graph Neural Networks Go „Online"? An Analysis of Pretraining and Inference. *Representation Learning on Graphs and Manifolds Workshop @ ICLR*. [paper](https://rlgm.github.io/papers/21.pdf)
- Lukas Galke, Florian Mai, Ansgar Scherp (2019). What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [extended abstract]. *INFORMATIK*. [paper](https://doi.org/10.18420/inf2019_47)
- Ahmed Saleh, Tilman Beck, Lukas Galke, Ansgar Scherp (2018). Performance of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents. *ICADL*. [paper](https://doi.org/10.1007/978-3-030-04257-8_30)
- Lukas Galke, Florian Mai, Iacopo Vagliano, Ansgar Scherp (2018). Multi-Modal Adversarial Autoencoders for Recommendation of Citations and Subject Labels. *UMAP*. [paper](https://doi.org/10.1145/3209219.3209236)
- Anne Lauscher, Kai Eckert, Lukas Galke, Ansgar Scherp, Syed Tassen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Philipp Zumstein, Annette Klein (2018). Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph. *JCDL*. [paper](https://doi.org/10.1145/3197026.3197050)
- Florian Mai, Lukas Galke, Ansgar Scherp (2018). Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text. *JCDL*. [paper](https://doi.org/10.1145/3197026.3197039)
- Iacopo Vagliano, Lukas Galke, Florian Mai, Ansgar Scherp (2018). Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation. *RecSys Challenge Workshop @ RecSys*. [paper](https://doi.org/10.1145/3267471.3267476)
- Lukas Galke, Gunnar Gerstenkorn, Ansgar Scherp (2018). A Case Study of Closed-Domain Response Suggestion with Limited Training Data. *Text-based Information Retrieval Workshop @ DEXA*. [paper](https://doi.org/10.1007/978-3-319-99133-7_18) [code](https://github.com/lgalke/resuggest)
- Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunch, Ansgar Scherp (2017). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation. *K-CAP*. [paper](https://doi.org/10.1145/3148011.3148039)
- Lukas Galke, Ahmed Saleh, Ansgar Scherp (2017). Word Embeddings for Practical Information Retrieval. *INFORMATIK*. [paper](https://doi.org/10.18420/in2017_215) [code (>200 stars, >40 forks)](https://github.com/lgalke/vec4ir)

## Project reports

- Iacopo Vagliano, Till Blume, Lukas Galke, Florian Mai, Ahmed Saleh, Alexandros Pournaras, Nikolaos Gkalelis, Damianos Galanopoulos, Vasileios Mezaris, Ilija Šimić, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Andrea Zielinski, Peter Mutschke (2019). Deliverable 3.3: Technologies for MOVING data processing and visualisation v3.0. [report](http://moving-project.eu/wp-content/uploads/2019/03/moving_d3.3_v1.0.pdf)
- Iacopo Vagliano, Mohammad Abdel-Qader, Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Ilija Šimić, Cecilia di Sciascio, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke (2018). Technologies for MOVING data processing and visualisation v2.0. [report](http://moving-project.eu/wp-content/uploads/2018/03/moving_d3.2_v1.0.pdf)
- Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Matthias Schulte-Althoff, Chrysa Collyda, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Peter Hasitschka, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke Thomas Gottron (2017). Technologies for MOVING data processing and visualisation v1.0. [report](http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf)


---
title: Talks
---

## Invited talks

- [t39] Lukas Galke (2025, May 13). **Evaluating Large and Multilingual Language Models through Citizen Science**. AI, Citizen Science, and MedTech – An Exploration, SDU, Odense, Denmark.
- [t37] Lukas Galke (2024, July 4). **Machine Communication and the Emergence of Language**. Max Planck Proudly Presents, Nijmegen, Netherlands.
- [t36] Lukas Galke (2024, June 19). **Machine Communication**. IMADA Seminar, SDU, Odense, Denmark.
- [t35] Lukas Galke (2024, June 10). **Emergent communication and learning pressures in language models**. ANN Humlang workshop, Amsterdam.
- [t33] Lukas Galke (2024, May 3). **Learning Pressures and Inductive Biases in Neural Language Models**. Meeting of Creative Intelligence Lab, Leiden, Netherlands.
- [t29] Lukas Galke (2023, July 20). **What makes a language easy to deep-learn?**. Colloquium Cognitive Systems, University of Ulm, Germany.
- [t28] Lukas Galke (2023, July 10). **Lifelong Learning and Out-of-distribution Detection on Evolving Graphs**. VU AI Colloquium, Amsterdam, The Netherlands.
- [t27] Lukas Galke (2023, July 5). **Lifelong Neural Communication**. Data Science, Hamburg University, Germany.
- [t26] Lukas Galke (2023, May 17). **Uncovering Patterns in Medical Literature through Lifelong Automated Categorization and Research Dynamics Analysis with Machine Learning**. KIK AI Coffee & Learning meeting, Amsterdam University Medical Centre, The Netherlands.
- [t25] Lukas Galke (2023, May 16). **What makes a language easy to deep-learn?**. Computational Linguistics Seminar, University of Amsterdam, The Netherlands.
- [t24] Lukas Galke (2023, February 27). **What makes a language easy to deep learn?**. Language Evolution and Emergence meeting, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands.
- [t23] Lukas Galke (2022, October 20). **Structure in Language Learning Systems**. TALEP Marseille, France.

## Conference and workshop presentations

- [t42] Lukas Galke (2025, November). **LLM-Assisted Paper Reading**. Teaching and Learning Conference (TAL), SDU, Odense, Denmark.
- [t41] Lukas Galke (2025, September 19). **Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI**. FAIEMA Conference, Stavanger, Norway.
- [t40] Lukas Galke (2025, August). **Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?** [poster]. ACL 2025, Vienna, Austria.
- [t38] Lukas Galke (2024, July 9). **Harnessing Cross-lingual Morphological Generalization Abilities in Large Language Models with a Multilingual Wug Test** [poster]. Highlights in the Language Sciences Conference, Nijmegen.
- [t34] Lukas Galke (2024, May 20). **Learning Pressures and Inductive Biases in Emergent Communication: Parallels between Humans and Deep Neural Networks**. Evolang XV, May 20, 2024.
- [t31] Lukas Galke (2023, Oct 19) **Lifelong Learning for Evolving Graphs** [proposed talk]. ContinualAI Unconference, Online.
- [t30] Lukas Galke (2023, September 28). **What makes a language easy to deep-learn?**. Protolang 8, Rome, Italy.
- [t22] Lukas Galke (2022, September 5). **Machine Learning for Language Evolution -- What's missing?**. Machine Learning and the Evolution of Language (ml4evolang) workshop at JCOLE'22, Japan/online.
- [t19] Lukas Galke (2022, July 19). **General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings** [paper presentation (oral)]. World Congress on Computational Intelligence, Padua, Italy.
- [t18] Lukas Galke (2022, June 1). **Emergent Communication for Understanding Human Language Evolution: What’s missing?** [paper presentation (poster)]. IMPRS conference, Nijmegen, The Netherlands.
- [t17] Lukas Galke (2022, May 24). **Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP** [paper presentation (oral)]. ACL 2022. [video](https://doi.org/10.48448/09t2-ck98)
- [t16] Lukas Galke (2022, April 29). **Emergent Communication for Understanding Human Language Evolution** [paper presentation (oral) and discussion session]. EmeCom workshop at ICLR'22, online.
- [t15] Lukas Galke (2021, December 15). **COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics** [paper presentation (oral)]. 2021 IEEE International Conference on Big Data (Big Data), online.
- [t14] Lukas Galke (2021, July 18). **Lifelong learning of graph neural networks for open-world node classification** [paper presentation (oral)]. International Joint Conference on Neural Networks, online.
- [t12] Lukas Galke (2019, September 26). **Inductive Learning of Concept Representations from Library-Scale Corpora with Graph Convolution** [paper presentation (oral)], INFORMATIK 2019, Kassel, Germany.
- [t11] Lukas Galke (2019, September 26). **What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function** [paper presentation (oral)]. INFORMATIK 2019, Kassel, Germany
- [t10] Lukas Galke, Florian Mai (2019, May 8). **CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model** [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.
- [t9] Lukas Galke (2019, May 7). **Can Graph Neural Networks Go Online? An Analysis of Pretraining and Inference** [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.
- [t8] Anne Lauscher, Lukas Galke, Syed Tahseen Raza Rizvi (2018, November 6). **LOC-DB Evaluation: criteria and preliminary results**. Second Linked Open Citation Database (LOC-DB) workshop. [project presentation]
- [t7] Lukas Galke (2018, July 10). **Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels** [paper presentation (oral)]. Conference on User Modeling, Adaptation and Personalization, Singapore, Singapore.
- [t6] Lukas Galke (2017, December 6). **Using Titles vs. Full-text as Source for Automated Semantic Document Annotation** [paper presentation (oral)], Knowledge Capture Conference, Austin, Texas.
- [t5] Kai Eckert, Anne Lauscher, Lukas Galke (2017, November 7). **LOC-DB Konzepte**. First Linked Open Citation Database (LOC-DB) Workshop. [project presentation]
- [t4] Lukas Galke (2017, September 28). **Reranking-based Recommender Systems with Deep Learning** [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.
- [t3] Lukas Galke (2017, September 28). **Word Embeddings for Practical Information Retrieval** [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.
- [t2] Lukas Galke (2017, May 5). **Embedded Retrieval: Word Embeddings for Practical Information Retrieval** [M.Sc. thesis presentation]. Second DyESE workshop, Kiel, Germany.
- [t1] Lukas Galke (2016, September 16). **Information Retrieval on Sparse Data** [concept presentation]. First DyESE workshop, Oslo, Norway.

## Other

- [t21] Lukas Galke (2022, August 28). **Representation Learning for Texts and Graphs** [viva]. Department of Computer Science, Kiel University, Germany.
- [t20] Lukas Galke (2022, August 18). **Language Technology** [gave a 2x45min workshop to a broad audience]. Regiodag 2022 Probusclub Nijmegen e.o., Nijmegen, The Netherlands.
- [t13] Lukas Galke (2020, June 23). **Scaling Up Graph Neural Networks** [literature review talk], Graph Neural Networks Reading Group, online.


---
title: Teaching
---

*SDU students can find the material on [itslearning](http://sdu.itslearning.com/)*

- AI509: **Natural Language Processing** -- Fall 2025, SDU (main lecturer)
- AI508: **Computer Vision** -- Fall 2025, SDU (co-lecturer)
- DSK809: **Deep Learning** -- Fall 2025, SDU (responsible)
- AI506: **Advanced Machine Learning** -- Spring 2025, SDU (main lecturer)
- DM873/DS809: **Deep Learning** -- Fall 2024, SDU (main lecturer)



---
title: "About"
---

 <figure style="display: inline-block; margin: 0; max-width: 256px">
<img src="assets/images/2024-08-LGalke-sw-sq-max-full-neutral.jpeg" alt="A photo of Lukas Galke" style="width: 256px; height: 256px; object-fit: cover;"/>
<figcaption style="text-align: center; font-weight: 100;">Lukas Galke (Photo: <a href="https://www.instagram.com/paulgeorggalke/">paulgeorggalke</a>)</figcaption>
</figure>


## Bio


I am an Assistant Professor of Data Science and Advanced Machine Learning at the University of Southern Denmark in Odense. Before that, I was a postdoctoral researcher at the Max Planck Institute for Psycholinguistics in Nijmegen, Netherlands. I received my B.Sc. (2013), M.Sc. (2017), and PhD (2022), all in computer science, from Kiel University, Germany.

My current research focuses on mechanistic interpretability, AI safety, and the intersection of language models with cognitive science.

## Profiles

- ORCID: [0000-0001-6124-1092](https://orcid.org/0000-0001-6124-1092)
- DBLP: [Lukas Galke](https://dblp.org/pid/200/7830.html)
- Google Scholar: [Lukas Galke](https://scholar.google.de/citations?hl=en&pli=1&user=AHGGdYQAAAAJ)
- GitHub: [lgalke](https://github.com/lgalke)
- Mastodon: [@lpag@sigmoid.social](https://sigmoid.social/@lpag)
- LinkedIn: [Lukas Galke](https://www.linkedin.com/in/lukas-galke-8086b0155/)
- BlueSky: [@lukasgalke.bsky.social](https://bsky.app/profile/lukasgalke.bsky.social)
- X (inactive): [@LukasGalke](https://x.com/LukasGalke)

## Get to know me

- I am motivated by challenges.
- I practice Yoga.
- I like to go surfing and stand-up paddling.
- I like reading.  Current book recommendations: *The Light Pirate* by Lily Brooks-Dalton, *Station 11* by Emily St. John Mandel.
- I like to play chess. 
- I like photography, sometimes with drones.
- I make music. I play Saxophone since childhood, but only at Christmas these days. I am learning to play Ukulele (on hold).

## Some photos

![Dreamy Place, Sweden, 2022](assets/images/DC71A997-35F8-476D-BB97-D94680B51897.jpeg)

![Beach Mood, Voropør, Denmark, June 2020](assets/images/2020-07-Denmark-beach-mood-Voropor.jpeg)

![Hiking in Blaubeuren, Germany, July 2023](assets/images/2023-07-Germany_hiking-in-Blaubeuren.jpeg)

![Secret Island, Sweden, June 2022](assets/images/2022-06-Sweden_secret-island_Spatz-1_LG.jpg)

![Yoga on a Rock, Sweden, July 2022](assets/images/2022-07-Sweden_yoga-on-a-rock_Spatz-1_LG.jpg)

![North Sea meets Baltic Sea, Skagen, Denmark, August 2021](assets/images/2021-08-Denmark_North-Sea-meets-Baltic-Sea_LG.jpeg)

![Kruemel Robusto in Grena, Denmark, August 2021](assets/images/2021-08-Denmark-Grena_Kruemel-Robusto_LG.jpeg)

![Day trip to Venice, Italy, July 2022](assets/images/2022-07-Italy_day-trip-to-Venice_LG.jpeg)

![Sit-down Paddling in Germany, June 2021](assets/images/2021-06-Germany-SUP.jpeg)



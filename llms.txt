
Name: Lukas Paul Achatius Galke

Position: Assistant Professor of Data Science and Advanced Machine Learning

Affiliation: 
University of Southern Denmark (SDU)
Department of Mathematics and Computer Science (IMADA), Section for Data Science and Statistics
Centre for Machine Learning

Research Interests: 
- Machine learning
- Natural language processing
- Machine communication
- Continual learning
- Learning on graphs

Email: galke@imada.sdu.dk

Website: https://lgalke.github.io/

## Bio

Since 2024, I am an Assistant Professor of Data Science and Advanced Machine Learning. Before that, I was a postdoctoral researcher at the Max Planck Institute for Psycholinguistics in Nijmegen, Netherlands. I've received my B.Sc. (2013), M.Sc. (2017), and PhD degree (2022), all in computer science, from Kiel University, Germany. My research interests lie in machine learning and natural language processing, aiming to make neural network models more interpretable and trustworthy.

## Profiles

- ORCID: [0000-0001-6124-1092](https://orcid.org/0000-0001-6124-1092)
- DBLP: [Lukas Galke](https://dblp.org/pid/200/7830.html)
- Google Scholar: [Lukas Galke](https://scholar.google.de/citations?hl=en&pli=1&user=AHGGdYQAAAAJ)
- GitHub: [lgalke](https://github.com/lgalke)
- Mastodon: [@lpag@sigmoid.social](https://sigmoid.social/@lpag)
- LinkedIn: [Lukas Galke](https://www.linkedin.com/in/lukas-galke-8086b0155/)
- BlueSky: [@lukasgalke.bsky.social](https://bsky.app/profile/lukasgalke.bsky.social)
- X (inactive): [@LukasGalke](https://x.com/LukasGalke)



---
title: CV
---

## General Information

**Name**: Lukas Paul Achatius Galke

**Address**: Campusvej 55, 5230 Odense, Denmark

**Office**: Ø12-509b-2

## Experience

2024--present: **Assistant Professor**, University of Southern Denmark (SDU), Department of Mathematics and Computer Science (IMADA), Section for Data Science and Statistics (DSS), Centre for Machine Learning (C4ML)

2022-2024: **Postdoctoral Researcher**, Max Planck Institute for Psycholinguistics

2017-2022: **Doctoral Researcher**, Kiel University & ZBW

## Education

2022: PhD in Computer Science, Kiel University, Germany

2017: M.Sc. in Computer Science, Kiel University, Germany

2013: BSc in Computer Science, Kiel University, Germany

## Teaching experience as main lecturer

2025, Spring: Advanced Machine Learning, SDU

2024, Autumn: Deep Learning, SDU

## Selected publications

- Galke, L., Ram, Y., Raviv, L. (2024) Deep neural networks and humans both benefit from compositional language structure. Nature Communications 15:10816.
- Galke, L., Vagliano, I., Franke, B., Zielke, T., Hoffmann, M., Scherp A. (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176.
- Galke, L., Scherp, A. (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. ACL 2022, pages 4038–4051.

## Selected invited talks

- Emergent communication and learning pressures in language models. Workshop on Using Artificial Neural Networks for Studying Human Language Learning and Processing, 2024, June 10, University of Amsterdam.
- Lifelong Learning and Out-of-distribution Detection on Evolving Graphs. VU AI Colloquium, 2023, July 10, Vrije Universiteit Amsterdam, Netherlands.
- What makes a language easy to deep-learn? Computational Linguistics Seminar, 2023, May 16, University of Amsterdam,
Netherlands.

## Academic service

- **Program Committee**: ACL, CogSci, CIKM, ECAI, EMNLP, ICLR
- **Reviewer**: Artificial Intelligence Review, IEEE Access, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Knowledge and Data Engineering, Information Processing and Management, Journal of Artificial Intelligence Reseearch, Journal of Web Semantics, Knowledge-based Systems, Nature Communications, Nature Scientific Reports, Neural Networks, Pattern Recognition


---
title: Projects
---

## Interpretability of Language Models {#mc-interpretability}

*Since 2023*

Large language models have shown incredible results and experienced a rapid
widespread adoption. However, we do not know *how* the trained models come to
their decisions of what text to generate. Therefore, this project aims to
investigate the interpretability of language models through structural probing,
behavioral probing, and mechanistic interpretability -- and by bringing theories
and experimental paradigms from psycholinguistics into machine learning.

- Paper: [**Deep neural networks and humans both benefit from compositional language structure**](https://rdcu.be/d5f2e). *Nature Communications* 15:10816. 
- Paper: [Learning and communication pressures in neural networks: Lessons from emergent communication](https://doi.org/10.34842/3vr5-5r49). *Language Development Research* 5(1).  
- Paper: [Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test](https://aclanthology.org/2024.cmcl-1.15/) published in the [CMCL workshop](https://cmclorg.github.io) at the [ACL 2024 conference](https://2024.aclweb.org).
- Poster at the [Highlights in the Language Sciences conference](https://hils2024.nl), July 8--11, 2024
- Poster at the workshop on [Using Artificial Neural Networks for Studying Human Language Learning and Processing](https://ann-humlang.github.io), June 10--12, 2024
- Extended abstract on Testing the Linguistic Niche Hypothesis in Large Language Models with a Multilingual Wug Test published in the [Proceedings of Evolang XV](https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3587960) (pp. 91--94)
- Podium presentation by Anh Dang at [Evolang 2024](https://evolang2024.github.io)
- Talk at the [Protolang 8 conference](https://sites.google.com/view/protolang8/home), Rome, September 27--28, 2023. Abstract published in the [Protolang 8 book of abstracts](https://drive.google.com/file/d/1rbBGo82zV4nCNgRiLeGXEJoR79wSGxpu/view)

## Machine Communication and the Emergence of Language {#mc-emecom}

*Since 2022*

Imagine you put artificial neural network agents into a box and give them a task
that can only be solved with communication. How do neural network agents learn
to communicate? What communication protocols emerge? What are the parallels to
human communication?

- Machine Communication and the Emergence of Language. Talk given at the MPI Proudly Presents series, July 4, 2024, [Max Planck Institute for Psycholinguistics](https://www.mpi.nl), Nijmegen, NL.
- Invited talk at the workshop on [Using Artificial Neural Networks for Studying Human Language Learning and Processing](https://ann-humlang.github.io), June 10--12, 2024
- [Machine Learning and the Evolution of Language workshop at the Joint Conference on Language Evolution](https://ml4evolang.github.io)
- [Emergent Communication workshop paper at ICLR](https://openreview.net/forum?id=rqUGZQ-0XZ5)



## What Matters for Text Classification (textclf) {#textclf}

*Since 2021*

Automatically categorizing text documents is a popular research topic with
numerous new approaches being published each year. However, do they really give
an advantage over earlier approaches? This question has triggered this line of
research and the answer we have is worrisome. For instance, many recently
proposed methods such as TextGCN are outperformed by a simple multilayer
perceptron on a bag-of-words representation -- a decade old technique enhanced
with today's best practices.

Even in the era of language models, the results on text classification are sometimes counter-intuitive.
As such, fine-tuned small models typically outperform large language models.

- [Simplifying hierarchical text classifciation](https://ebooks.iospress.nl/doi/10.3233/FAIA240661)
- [preprint of a comparative survey paper](https://arxiv.org/abs/2204.03954)
- [ACL 2022 paper](https://doi.org/10.18653/v1/2022.acl-long.279)
- [code for multi-label classification](https://github.com/drndr/multilabel-text-clf)
- [code for single-label classification](https://github.com/lgalke/text-clf-baselines)
- [extra code for single-label classification with different methods](https://github.com/FKarl/text-classification)


## Lifelong Graph Representation Learning (LGL) {#LGL}

*Since 2019*

Graph neural networks have quickly risen to be the standard technique for machine learning on graph-structured data.  Yet graph neural networks are usually only applied to static snapshots of graphs, while real-world graphs (social media, publication networks) are continually evolving.  Evolving graphs come with challenges that are rarely reflected in the graph representation learning literature, such as dealing with new classes in node classification.  We pursue a lifelong learning approach for graph neural networks on evolving graphs and investigate incremental training, out-of-distribution detection, and issues caused by an imbalanced class distribution.

- [CoLLAs 2024 conference paper on zero-shot learning in graphs](https://lifelong-ml.cc/Conferences/2024/acceptedpapersandvideos/conf-2024-38) or on [arXiv](https://arxiv.org/abs/2406.09926)
- [Code for CoLLAs 2024 paper](https://github.com/Bobowner/POWN)
- [IJCNN 2023 conference paper on new class detection in graphs](https://doi.org/10.1109/IJCNN54540.2023.10191071)
- [*Neural Networks* journal paper, 2023](https://pure.mpg.de/rest/items/item_3368482_4/component/file_3510107/content)
- [IJCNN 2021 conference paper](https://doi.org/10.1145/3197026.3197050)
- [ICLR 2019 workshop paper](https://rlgm.github.io/papers/21.pdf)
- [Code for 2023 *Neural Networks* paper](https://github.com/lgalke/lifelong-learning)
- [Code for IJCNN 2023 paper](https://github.com/Bobowner/Open-World-LGL)
- [Code for ICLR 2019 workshop paper](https://github.com/lgalke/gnn-pretraining-evaluation)
- related project: [Q-AKTIV](#q-aktiv)


## Analyzing the Scientific, Economic and Societal Impact of Research Activities and Research Networks (Q-AKTIV) {#q-aktiv}

*2019--2022, funded by the Federal Ministry of Education and Research (BMBF)*

The aim of Q-AKTIV is to improve the methods for forecasting dynamics and interactions between research, technology development, and innovation. The network analysis methods will be based on recent developments in Deep Learning. In addition to the emergence of new knowledge areas and networks, we focus on the convergence processes of established sectors. The development and evaluation of the new methods initially takes place in the field of life sciences, which is characterized by high marked dynamics. The additional application in economics enables a systematic comparison of the dynamics between the disciplines of science. The new methods will be used to predict the impact of existing research and network structures on the dynamics of knowledge and technologies as well as the future relevance of topics and actors. The result of Q-AKTIV is an implemented and evaluated instrument for the strategic analysis and prognosis of the dynamics in science and innovation. This complements today’s primarily qualitative approaches to early strategic planning and increases the decision-making ability of research institutions, policy makers, and industries. In addition to the analysis of dynamics, also valuable indicators for R & D performance measurement can be derived, e.g., the registration of patents based on scientific publications, the economic development of the companies involved, as well as the outreach of research activities. The practice partner brings in the necessary experience in the field of business valuation and strategy development and ensures a practical testing of the toolkit.

- [Scientometrics journal paper on the lack of interdisciplinarity in the scientific response to COVID-19, 2024](https://doi.org/10.1007/s11192-024-05132-x)
- [TEM journal paper evaluating the developed methods, 2023](https://doi.org/10.1109/TEM.2023.3308008)
- [project website](https://q-aktiv.github.io)
- [COVID-19++ dataset](https://doi.org/10.5281/zenodo.5531084)
- [COVID-19++ dataset paper](https://doi.org/10.1109/BigData52589.2021.9671730).
- [conference paper on learning concept representations](https://doi.org/10.18420/inf2019_26)
- [follow-up work building on concept representation learning methods](https://doi.org/10.1111/jpim.12572)
- [Python package for learning concept representations and analyzing network dynamics](https://gitlab.com/Q-Aktiv/qgraph)
- [workshop paper on data enrichment 2](https://ceur-ws.org/Vol-2773/paper-09.pdf)
- [workshop paper on data enrichment 1](https://ceur-ws.org/Vol-2451/paper-23.pdf)
- [tools for harvesting raw data](https://github.com/Q-AKTIV/covid19-harvesting-tools)
- [interview about open science tools for digital collaboration (en)](https://www.zbw-mediatalk.eu/en/2019/03/praxisbericht-open-science-diese-tools-foerdern-die-zusammenarbeit/)
- [interview about open science tools for digital collaboration (de)](https://www.zbw-mediatalk.eu/de/2019/03/praxisbericht-open-science-diese-tools-foerdern-die-zusammenarbeit/)
- [journal paper (German)](https://www.universitaetsverlagwebler.de/_files/ugd/7bac3c_24fe9adc2e3740178ad5ba98f66d1931.pdf)
- [press item (German)](https://www.b-i-t-online.de/neues/5386)
- [press release (ZB MED, German)](https://www.zbmed.de/forschen/abgeschlossene-projekte/q-aktiv)
- [funding announcement (German)](https://www.wihoforschung.de/wihoforschung/de/bmbf-projektfoerderung/foerderlinien/quantitative-wissenschaftsforschung/q-aktiv/q-aktiv_node.html)
- [final project report (German)](https://zenodo.org/records/5788648)

## Representation Learning for Texts and Graphs {#phd-project}

*2017-2022*

My PhD project. A meta-project bringing together [word2mat](#word2mat),
[textclf](#textclf), [aaerec](#aaerec), [LGL](#LGL). [phd
thesis](https://doi.org/10.21941/kcss/2023/1)

## Word Matrices for Text Representation Learning (word2mat) {#word2mat}

*2017-2022*

The idea of this project was to embed each word as a matrix as an alternative
to vectors used in word embeddings. By using matrix embeddings instead of
vector embeddings, we can use matrix multiplication as a composition function
to form a representation for phrases and sentences. While word matrices alone
did not exceed the performance of word vectors, a combination of word matrices
and word vectors turned out to be beneficial. Later, we showed that pre-trained
language models can be distilled into such a purely embedding-based model,
giving benefits in efficiency while keeping reasonable accuracy.

- [IJCNN paper on distilling the knowledge from a pre-trained language model into matrix embedding models](https://doi.org/10.1109/IJCNN55064.2022.9892144)
- [extended abstract in the best-of data science track at the INFORMATIK 2019](https://doi.org/10.18420/inf2019_47)
- [ICLR paper on self-supervised learning of word matrices](https://openreview.net/pdf?id=H1MgjoR9tQ)
- [code](https://github.com/florianmai/word2mat)

## Autoencoders for Document-based Recommendations (aaerec) {#aaerec}

*2017--2022*

The aim of this project was to build a document-level citation recommendation
system that could, for example, make users aware of missing references.  A
specialty of this project compared to other recommender systems is that we do
not use any a user data or a user profile but only operate on the contents of
the current draft. The main research question was whether models could be
enhanced by using textual side information, such as the title of the draft,
which we confirmed for a wide range of autoencoder-based recommendation models.
Interestingly, we found that the choice of the best model depends on the
semantics of item co-occurrence.  When item co-occurrence implies relatedness
(as in citations), looking at other items is far more useful than looking at
the text. In contrast, when item co-occurrence implies diversity, such as in
subject labels from professional subject indexers, the text is more useful.

- [journal paper on citation and subject label recommendation](https://doi.org/10.1007/s10791-022-09408-9)
- [conference paper on citation and subject label recommendation](https://dl.acm.org/doi/abs/10.1145/3209219.3209236)
- [RecSys challenge workshop paper on music recommendation for automatic playlist continuation](https://doi.org/10.1145/3267471.3267476)
- [codebase for all three papers](https://github.com/lgalke/aae-recommender)

## Linked Open Citation Database (LOC-DB) {#loc-db}

*2017--2020, funded by Deutsche Forschungsgemeinschaft (DFG)*

The LOC-DB project will develop ready-to-use tools and processes based on the
linked-data technology that make it possible for a single library to
meaningfully contribute to an open, distributed infrastructure for the
cataloguing of citations. The project aims to prove that, by widely
automating cataloguing processes, it is possible to add a substantial benefit
to academic search tools by regularly capturing citation relations. These
data will be made available in the semantic web to make future reuse
possible. Moreover, we document effort, number and quality of the data in a
well-founded cost-benefit analysis.  The project will use well-known methods
of information extraction and adapt them to work for arbitrary layouts of
reference lists in electronic and print media. The obtained raw data will be
aligned and linked with existing metadata sources. Moreover, it will be shown
how these data can be integrated in library catalogues. The system will be
deployable to use productively by a single library, but in principle it will
also be scalable for using it in a network.

- [conference paper on citation recommendation](https://dl.acm.org/doi/abs/10.1145/3209219.3209236)
- [main paper on the project as a whole](https://dl.acm.org/doi/abs/10.1145/3197026.3197050)
- [demo of the LOC-DB project outcome](https://locdb.bib.uni-mannheim.de/demo-frontend/frontpage)
- [collection of code for the LOC-DB project](https://github.com/locdb)
- Second Linked Open Citation Database (LOC-DB) workshop, 2018, Mannheim, Germany.
- First Linked Open Citation Database (LOC-DB) workshop, 2017, Mannheim, Germany.
- [project website (de)](https://www.bib.uni-mannheim.de/ihre-ub/projekte-der-ub/linked-open-citation-database/)

## Word Embeddings for Information Retrieval (vec4ir) {#vec4ir}

*2016--2017*

The key idea was to use word embeddings for similarity scoring in information
retrieval. The two main take-aways:

1. It is important to retain the crisp matching operation (before similarity scoring), even when using word embeddings.
2. A combination of classic information retrieval method TF-IDF and word embeddings led to the best results.

- [INFORMATIK 2017 paper](https://doi.org/10.18420/in2017_215)
- [MSc thesis](/pdf/MSc-thesis_LG.pdf)
- [codebase for studying embedded retrieval (>200 stars, >40 forks)](https://github.com/lgalke/vec4ir)

## TraininG towards a society of data-saVvy inforMation prOfessionals to enable open leadership INnovation (MOVING)

*2016--2019, EU funding, Grant Agreement Number 693092*

I engaged in this EU Horizon 2020 project as a research assistant between 2016 and 2017, leading to contributions to the deliverables 3.1, 3.2 and 3.3, as well as various conference and workshop papers:

- [Deliverable 3.3](http://moving-project.eu/wp-content/uploads/2019/03/moving_d3.3_v1.0.pdf)
- [Deliverable 3.2](http://moving-project.eu/wp-content/uploads/2018/03/moving_d3.2_v1.0.pdf)
- [Deliverable 3.1](http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf)
- [ICADL 2018 paper on information retrieval on titles vs. full-text](https://doi.org/10.1007/978-3-030-04257-8_30)
- [DEXA 2018 workshop paper on response suggestion](https://doi.org/10.1007/978-3-319-99133-7_18)
- [code for response suggestion](https://github.com/lgalke/resuggest)
- [Project website](http://moving-project.eu)

## Extreme Multi-label Text Classification (Quadflor) {#xlmc}

*2015--2018*

This project originated from my Master's project (2015--2016), where we
developed a pipeline for extreme (=many possible classes) multi-label
text classification.  We found that a multi-layer perceptron beats the
state-of-the-art kNN approach by more than 30%.  Moreover, we compared using
either the full-text or only the title of a research paper as a basis for
classification. The result was that the full-text is only marginally better
than the title. My team member Florian Mai investigated the trade-off between
full-text and title in his Master's thesis, finding that the increased
availability of title data compensates for increased information in full-text
articles.

- [code](https://github.com/quadflor/Quadflor) (bought by [ZBW](https://zbw.eu) for production usage)
- [K-CAP 2017 paper](https://doi.org/10.1145/3148011.3148039) (outcome of the Master's project)
- [JCDL 2018 paper](https://doi.org/10.1145/3197026.3197039) (outcome of Florian's Master's thesis)


---
title: Publications
---

## Upcoming journal articles

- [j8] Lukas Galke, Andor Diera, Bao Xin Lin, Bhakti Khera, Tim Meuser, Tushar Singal, Fabian Karl, Ansgar Scherp (in revision). Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single-label and Multi-label Text Classification. [preprint](https://arxiv.org/abs/2204.03954).

## Journal articles

- [j7] Lukas Galke, Limor Raviv (2025). Learning and communication pressures in neural networks: Lessons from emergent communication. *Language Development Research* 5(1). [paper](https://doi.org/10.34842/3vr5-5r49) [preprint](https://arxiv.org/abs/2403.14427)
- [j6] Lukas Galke, Yoav Ram, Limor Raviv (2024). Deep neural networks and humans both benefit from compositional language structure. *Nature Communications* 15:10816 [paper](https://rdcu.be/d5f2e) [code](https://github.com/lgalke/easy2deeplearn) [data](https://doi.org/10.5281/zenodo.14205452)
- [j5] Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (2024). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19. *Scientometrics*. [paper](https://doi.org/10.1007/s11192-024-05132-x)
- [j4] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad U. Förstner, Klaus Tochtermann, Carsten Schultz (2023). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence. *IEEE Transactions on Engineering Management*. [paper](https://doi.org/10.1109/TEM.2023.3308008)
- [j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. *Neural Networks* 164, 156-176. [paper](https://pure.mpg.de/rest/items/item_3368482_4/component/file_3510107/content) [code](https://github.com/lgalke/lifelong-learning)
- [j2] Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. *Inf Retrieval J* 25, 269–305. [paper](https://doi.org/10.1007/s10791-022-09408-9) [code](https://github.com/lgalke/aae-recommender)
- [j1] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement [translated: Early-detection of scientic convergence in university management]. *Hochschulmanagement* 16(1). [complete issue](https://www.universitaetsverlagwebler.de/_files/ugd/7bac3c_24fe9adc2e3740178ad5ba98f66d1931.pdf)

## Conference papers

- [c15] Andor Diera, Lukas Galke, and Ansgar Scherp (2025). Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search. *European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2025). [preprint](https://arxiv.org/abs/2411.17538) [code](https://github.com/drndr/code_isotropy)
- [c14] Jacob Nielsen, Lukas Galke, and Peter-Schneider Kamp (2025). When are 1.58 bits enough? A Bottom-up Exploration of Quantization-aware Training with Ternary Weights. *17th International Conference on Agents and Artificial Intelligence (ICAART 2025)*. [preprint](https://arxiv.org/abs/2411.05882)
- [c13] Yousef Younes, Lukas Galke, and Ansgar Scherp (2024). RADAr: A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification. *27th European Conference on Artificial Intelligence 2024 (ECAI 2024)*. [paper](https://ebooks.iospress.nl/doi/10.3233/FAIA240661) [code](https://github.com/yousef-younes/RADAr)
- [c12] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2024). POWN: Prototypical Open-world Node Classification.  *Conference on Lifelong Learning Agents (CoLLAs 2024). [paper](https://lifelong-ml.cc/Conferences/2024/acceptedpapersandvideos/conf-2024-38) [code](https://github.com/Bobowner/POWN)
- [c11] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2023). Open-World Lifelong Graph Learning. In *International Joint Conference on Neural Networks (IJCNN 2023)*. IEEE. [paper](https://doi.org/10.1109/IJCNN54540.2023.10191071) [code](https://github.com/Bobowner/Open-World-LGL)
- [c10] Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand Nölscher, Angelina Sonderecker, Ansgar Scherp (2022). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. In *Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN 2022)*. IEEE. [paper](https://doi.org/10.1109/IJCNN55064.2022.9892144)
- [c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics. [paper](https://doi.org/10.18653/v1/2022.acl-long.279) [code](https://github.com/lgalke/text-clf-baselines)
- [c8] Lukas Galke, Benedikt Franke, Tobias Zielke, Ansgar Scherp (2021). Lifelong Learning of Graph Neural Networks for Open-World Node Classification. In *Proceedings of the 2021 International Joint Conference on Neural Networks (IJCNN)*. IEEE. [paper](https://doi.org/10.1109/IJCNN52387.2021.9533412) [code](https://github.com/lgalke/lifelong-learning)
- [c7] Lukas Galke, Tetyana Melnychuk, Eva Seidlmayer, Steffen Trog, Konrad U. Förster, Carsten Schultz, Klaus Tochtermann (2019). Inductive Learning of Concept Representations from Library-Scale Bibliographic Corpora GI. [paper](https://doi.org/10.18420/inf2019_26)
- [c6] Florian Mai, Lukas Galke, Ansgar Scherp (2019). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model. In *Proceedings of the Seventh International Conference on Learning Representations (ICLR)*. OpenReview.net. [paper](https://openreview.net/pdf?id=H1MgjoR9tQ) [code](https://github.com/florianmai/word2mat)
- [c5] Ahmed Saleh, Tilman Beck, Lukas Galke, Ansgar Scherp (2018). Performance of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents. In *Proceedings of the International Conference on Asian Digital Libraries (ICADL)*. [paper](https://doi.org/10.1007/978-3-030-04257-8_30)
- [c4] Lukas Galke, Florian Mai, Iacopo Vagliano, Ansgar Scherp (2018). Multi-Modal Adversarial Autoencoders for Recommendation of Citations and Subject Labels. In *Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization (UMAP)*. ACM. [paper](https://doi.org/10.1145/3209219.3209236)
- [c3] Anne Lauscher, Kai Eckert, Lukas Galke, Ansgar Scherp, Syed Tassen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Philipp Zumstein, Annette Klein (2018). Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph. In *Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL)*. ACM. [paper](https://doi.org/10.1145/3197026.3197050)
- [c2] Florian Mai, Lukas Galke, Ansgar Scherp (2018). Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text. In *Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL)*. ACM. [paper](https://doi.org/10.1145/3197026.3197039)
- [c1] Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunch, Ansgar Scherp (2017). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation. In *Knowledge Capture Conference (K-CAP)*. ACM. [paper](https://doi.org/10.1145/3148011.3148039)

## Workshop papers

- [w10] Anh Dang, Limor Raviv, Lukas Galke (2024). Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test. In *Cognitive Modeling and Computational Linguistics Workshop at ACL*. [paper](https://aclanthology.org/2024.cmcl-1.15/)
- [w9] Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp (2023). GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. GenBench Workshop @ EMNLP 2023. [paper](https://aclanthology.org/2023.genbench-1.2/)
- [w8] Lukas Galke, Yoav Ram, Limor Raviv (2022). Emergent communication for understanding human language evolution: What's missing?. Emergent Communication workshop at *Tenth International Conference on Learning Representations (ICLR 2022)*. OpenReview.net. [paper](https://openreview.net/forum?id=rqUGZQ-0XZ5)
- [w7] Lukas Galke, Eva Seidlmayer, Gavin Lüdemann, Lisa Langnickel, Tetyana Melnychuk, Konrad U Förstner, Klaus Tochtermann, Carsten Schultz (2021). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics. In *2021 IEEE International Conference on Big Data (Big Data)*. IEEE. [paper](https://doi.org/10.1109/BigData52589.2021.9671730)
- [w6] Eva Seidlmayer, Jakob Voß, Tatyana Melnychuk, Lukas Galke, Klaus Tochtermann, Carsten Schultz, Konrad U. Förstner (2020). ORCID for Wikidata — Data enrichment for scientometric applications, Wikidata workshop @ *ISWC 2020*. [paper](https://ceur-ws.org/Vol-2773/paper-09.pdf)
- [w5] Eva Seidlmayer, Lukas Galke, Tatyana Melnychuk, Carsten Schultz, Klaus Tochtermann, Konrad U. Förstner (2019): Take it Personally — A Python library for enrichment in informetrical applications. Posters&Demos @ *SEMANTICS 2019*. [paper](https://ceur-ws.org/Vol-2451/paper-23.pdf)
- [w4] Lukas Galke, Iacopo Vagliano, Ansgar Scherp (2019). Can Graph Neural Networks Go „Online“? An Analysis of Pretraining and Inference. Representation Learning on Graphs and Manifolds workshop @ *ICLR 2019*. [paper](https://rlgm.github.io/papers/21.pdf)
- [w3] Iacopo Vagliano, Lukas Galke, Florian Mai, Ansgar Scherp (2018). Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation. RecSys Challenge workshop @ *RecSys'18*. [paper](https://doi.org/10.1145/3267471.3267476)
- [w2] Lukas Galke, Gunnar Gerstenkorn, Ansgar Scherp (2018). A Case Study of Closed-Domain Response Suggestion with Limited Training Data. Text-based Information Retrieval workshop @ *DEXA'18*. [paper](https://doi.org/10.1007/978-3-319-99133-7_18) [code](https://github.com/lgalke/resuggest)
- [w1] Lukas Galke, Ahmed Saleh, Ansgar Scherp (2017). Word Embeddings for Practical Information Retrieval. In *INFORMATIK 2017*. Gesellschaft für Informatik, Bonn. (S. 2155-2167). [paper](https://doi.org/10.18420/in2017_215) [code (>200 stars, >40 forks)](https://github.com/lgalke/vec4ir)

## (Extended) abstracts

- [a3] Lukas Galke, Yoav Ram, Limor Raviv (2023). What makes a languagae easy to deep-learn? [1-page abstract]. *Protolang 8*, 2023.
- [a2] Lukas Galke (2022). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability [selected PhD thesis abstract]. *IEEE Intelligent Informatics Bulletin*, 22(1), 52. [complete issue](https://www.comp.hkbu.edu.hk/~cib/2022/IIB2022_Final.pdf)
- [a1] Lukas Galke, Florian Mai, Ansgar Scherp (2019): What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [extended abstract]. *INFORMATIK 2019*. GI. [paper](https://doi.org/10.18420/inf2019_47)

## Thesis

- Lukas Galke (2023). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability. Number 2023/1 in Kiel Computer Science Series. Department of Computer Science, 2023. Dissertation, Faculty of Engineering, Kiel University. [pdf](https://doi.org/10.21941/kcss/2023/1)

## Project reports

- [r3] Iacopo Vagliano, Till Blume, Lukas Galke, Florian Mai, Ahmed Saleh, Alexandros Pournaras, Nikolaos Gkalelis, Damianos Galanopoulos, Vasileios Mezaris, Ilija Šimić, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Andrea Zielinski, Peter Mutschke (2019). Deliverable 3.3: Technologies for MOVING data processing and visualisation v3.0. [report](http://moving-project.eu/wp-content/uploads/2019/03/moving_d3.3_v1.0.pdf)
- [r2] Iacopo Vagliano, Mohammad Abdel-Qader, Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Ilija Šimić, Cecilia di Sciascio, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke (2018). Technologies for MOVING data processing and visualisation v2.0. [report](http://moving-project.eu/wp-content/uploads/2018/03/moving_d3.2_v1.0.pdf)
- [r1] Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Matthias Schulte-Althoff, Chrysa Collyda, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis,  Peter Hasitschka, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke Thomas Gottron (2017). Technologies for MOVING data processing and visualisation v1.0. [report](http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf)



---
title: Talks
---

## Invited talks 

- [t37] Lukas Galke (2024, July 4). *Machine Communication and the Emergence of Language*. MPI Proudly Presents, Nijmegen.
- [t36] Lukas Galke (2024, June 19). *Machine Communication*. IMADA Seminar, SDU Odense.
- [t35] Lukas Galke (2024, June 10). *Emergent communication and
learning pressures in language models* [invited talk]. ANN Humlang workshop, Amsterdam.
- [t33] Lukas Galke (2024, May 3). *Learning Pressures and Inductive Biases in Neural Language Models* [invited talk]. Meeting of Creative Intelligence Lab, Leiden, Netherlands.
- [t29] Lukas Galke (2023, July 20). *What makes a language easy to deep-learn?* [invited talk]. Colloquium Cognitive Systems, University of Ulm, Germany.
- [t28] Lukas Galke (2023, July 10). *Lifelong Learning and Out-of-distribution Detection on Evolving Graphs* [invited talk], VU AI Colloquium, Amsterdam, The Netherlands.
- [t27] Lukas Galke (2023, July 5). *Lifelong Neural Communication* [informal invited talk]. Data Science, Hamburg University, Germany.
- [t26] Lukas Galke (2023, May 17). *Uncovering Patterns in Medical Literature through Lifelong Automated Categorization and Research Dynamics Analysis with Machine Learning* [informal invited talk]. KIK AI Coffee & Learning meeting, Amsterdam University Medical Centre, The Netherlands.
- [t25] Lukas Galke (2023, May 16). *What makes a language easy to deep-learn?* [invited talk]. Computational Linguistics Seminar, University of Amsterdam, The Netherlands.
- [t24] Lukas Galke (2023, February 27). *What makes a language easy to deep learn?* [invited talk]. Language Evolution and Emergence meeting, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands.
- [t23] Lukas Galke (2022, October 20). *Structure in Language Learning Systems* [informal invited talk]. TALEP Marseille, France.

## Conference and workshop presentations

- [t38] Lukas Galke (2024, July 9). *Harnessing Cross-lingual Morphological Generalization Abilities in Large Language Models with a Multilingual Wug Test* [poster]. Highlights in the Language Sciences Conference, Nijmegen.
- [t34] Lukas Galke (2024, May 20). *Learning Pressures and Inductive Biases in Emergent Communication: Parallels between
Humans and Deep Neural Networks*. Evolang XV, May 20, 2024.
- [t30] Lukas Galke (2023, September 28). What makes a language easy to deep-learn? Protolang 8, Rome, Italy. 
- [t22] Lukas Galke (2022, September 5). *Machine Learning for Language Evolution -- What's missing?*. Machine Learning and the Evolution of Language (ml4evolang) workshop at JCOLE'22, Japan/online.
- [t19] Lukas Galke (2022, July 19). *General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings* [paper presentation (oral)]. World Congress on Computational Intelligence, Padua, Italy.
- [t18] Lukas Galke (2022, June 1). *Emergent Communication for Understanding Human Language Evolution: What’s missing?* [paper presentation (poster)]. IMPRS conference, Nijmegen, The Netherlands.
- [t17] Lukas Galke (2022, May 24). *Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP* [paper presentation (oral)]. ACL 2022. [video](https://doi.org/10.48448/09t2-ck98)
- [t16] Lukas Galke (2022, April 29). *Emergent Communication for Understanding Human Language Evolution* [paper presentation (oral) and discussion session]. EmeCom workshop at ICLR'22, online.
- [t15] Lukas Galke (2021, December 15). *COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics* [paper presentation (oral)]. 2021 IEEE International Conference on Big Data (Big Data), online.
- [t14] Lukas Galke (2021, July 18). *Lifelong learning of graph neural networks for open-world node classification* [paper presentation (oral)]. International Joint Conference on Neural Networks, online.
- [t12] Lukas Galke (2019, September 26). *Inductive Learning of Concept Representations from Library-Scale Corpora with Graph Convolution* [paper presentation (oral)], INFORMATIK 2019, Kassel, Germany.
- [t11] Lukas Galke (2019, September 26). *What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function* [paper presentation (oral)]. INFORMATIK 2019, Kassel, Germany
- [t10] Lukas Galke, Florian Mai (2019, May 8). *CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model* [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.
- [t9] Lukas Galke (2019, May 7). *Can Graph Neural Networks Go Online? An Analysis of Pretraining and Inference* [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.
- [t8] Anne Lauscher, Lukas Galke, Syed Tahseen Raza Rizvi (2018, November 6). LOC-DB Evaluation: criteria and preliminary results. Second Linked Open Citation Database (LOC-DB) workshop. [project presentation]
- [t7] Lukas Galke (2018, July 10). *Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels* [paper presentation (oral)]. Conference on User Modeling, Adaptation and Personalization, Singapore, Singapore.
- [t6] Lukas Galke (2017, December 6). *Using Titles vs. Full-text as Source for Automated Semantic Document Annotation* [paper presentation (oral)], Knowledge Capture Conference, Austin, Texas.
- [t5] Kai Eckert, Anne Lauscher, Lukas Galke (2017, November 7). LOC-DB Konzepte. First Linked Open Citation Database (LOC-DB) Workshop. [project presentation]
- [t4] Lukas Galke (2017, September 28). *Reranking-based Recommender Systems with Deep Learning* [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.
- [t3] Lukas Galke (2017, September 28). *Word Embeddings for Practical Information Retrieval* [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.
- [t2] Lukas Galke (2017, May 5). *Embedded Retrieval: Word Embeddings for Practical Information Retrieval* [M.Sc. thesis presentation]. Second DyESE workshop, Kiel, Germany.
- [t1] Lukas Galke (2016, September 16). *Information Retrieval on Sparse Data* [concept presentation]. First DyESE workshop, Oslo, Norway.

## Other

- [t21] Lukas Galke (2022, August 28). *Representation Learning for Texts and Graphs* [viva]. Department of Computer Science, Kiel University, Germany.
- [t20] Lukas Galke (2022, August 18). *Language Technology* [gave a 2x45min workshop to a broad audience]. Regiodag 2022 Probusclub Nijmegen e.o., Nijmegen, The Netherlands.
- [t13] Lukas Galke (2020, June 23). *Scaling Up Graph Neural Networks* [literature review talk], Graph Neural Networks Reading Group, online.


---
title: Teaching
---

*SDU students can find the material on [itslearning](http://sdu.itslearning.com/)*

- AI506: **Advanced Machine Learning** -- Spring 2025, SDU
- DM873/DS809: **Deep Learning** -- Fall 2024, SDU


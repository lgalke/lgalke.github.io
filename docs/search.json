[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Bio",
    "section": "",
    "text": "If you need a bio or a picture of me, you can use the ones on this page."
  },
  {
    "objectID": "bio.html#formal-bio",
    "href": "bio.html#formal-bio",
    "title": "Bio",
    "section": "Formal bio",
    "text": "Formal bio\nLukas Galke was born in Wuppertal, Germany in 1989. He received the B.Sc. and M.Sc. degrees in computer science from Kiel University, Germany, in 2017 and the Ph.D. degree in computer science from the same university in 2023.\nFrom 2017 to 2021, he was a Doctoral Researcher with Kiel University and with ZBW – Leibniz Information Centre for Economics. Since 2022, he has been a Postdoctoral Researcher with the Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands. He is the author of more than 20 journal articles, conference proceedings papers, and workshop papers. His research interests include natural language processing, graph representation learning, and lifelong machine learning.\nDr. Galke is a member of the Gesellschaft für Informatik (GI) and Association for Computational Lingiustics (ACL) and was a recipient of the ACL Outstanding Reviewer Award in 2023."
  },
  {
    "objectID": "bio.html#less-formal-bio-100-words",
    "href": "bio.html#less-formal-bio-100-words",
    "title": "Bio",
    "section": "Less formal bio (100 words)",
    "text": "Less formal bio (100 words)\nLukas obtained his PhD in 2023 from Kiel University, specializing in Representation Learning for Texts and Graphs. Prior to that, he earned his MSc in Computer Science at the same institution in 2017. His research focuses on the intersection of lifelong machine learning and continuous or discrete communication in deep neural networks. His work touches various areas, including natural language processing, graph representation learning, and information retrieval. Since 2022, Lukas has been working as a postdoctoral researcher at the Max Planck Institute for Psycholinguistics, where he investigates the emergence of communication protocols between neural networks."
  },
  {
    "objectID": "bio.html#picture",
    "href": "bio.html#picture",
    "title": "Bio",
    "section": "Picture",
    "text": "Picture"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "[j8] Lukas Galke, Limor Raviv (under review). Emergent communication and learning pressures in language models: a language evolution perspective.\n[j7] Lukas Galke, Yoav Ram, Limor Raviv (under review). What makes a language easy to deep-learn? preprint\n[j6] Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (in revision). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19.\n[j5] Lukas Galke, Andor Diera, Bao Xin Lin, Bhakti Khera, Tim Meuser, Tushar Singal, Fabian Karl, Ansgar Scherp (under review). Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single-label and Multi-label Text Classification. preprint.\n[j4] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad U. Förstner, Klaus Tochtermann, Carsten Schultz (2023). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence. IEEE Transactions on Engineering Management. paper\n[j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176. paper code\n[j2] Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. Inf Retrieval J 25, 269–305. paper code\n[j1] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement [translated: Early-detection of scientic convergence in university management]. Hochschulmanagement 16(1). complete issue"
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "",
    "text": "[j8] Lukas Galke, Limor Raviv (under review). Emergent communication and learning pressures in language models: a language evolution perspective.\n[j7] Lukas Galke, Yoav Ram, Limor Raviv (under review). What makes a language easy to deep-learn? preprint\n[j6] Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (in revision). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19.\n[j5] Lukas Galke, Andor Diera, Bao Xin Lin, Bhakti Khera, Tim Meuser, Tushar Singal, Fabian Karl, Ansgar Scherp (under review). Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single-label and Multi-label Text Classification. preprint.\n[j4] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad U. Förstner, Klaus Tochtermann, Carsten Schultz (2023). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence. IEEE Transactions on Engineering Management. paper\n[j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176. paper code\n[j2] Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. Inf Retrieval J 25, 269–305. paper code\n[j1] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement [translated: Early-detection of scientic convergence in university management]. Hochschulmanagement 16(1). complete issue"
  },
  {
    "objectID": "publications.html#conference-papers",
    "href": "publications.html#conference-papers",
    "title": "Publications",
    "section": "Conference papers",
    "text": "Conference papers\n\n[c11] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2023). Open-World Lifelong Graph Learning. In International Joint Conference on Neural Networks (IJCNN). IEEE. paper code\n[c10] Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand Nölscher, Angelina Sonderecker, Ansgar Scherp (2022). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. In Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN). IEEE. paper\n[c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics. paper code\n[c8] Lukas Galke, Benedikt Franke, Tobias Zielke, Ansgar Scherp (2021). Lifelong Learning of Graph Neural Networks for Open-World Node Classification. In Proceedings of the 2021 International Joint Conference on Neural Networks (IJCNN). IEEE. paper code\n[c7] Lukas Galke, Tetyana Melnychuk, Eva Seidlmayer, Steffen Trog, Konrad U. Förster, Carsten Schultz, Klaus Tochtermann (2019). Inductive Learning of Concept Representations from Library-Scale Bibliographic Corpora GI. paper\n[c6] Florian Mai, Lukas Galke, Ansgar Scherp (2019). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model. In Proceedings of the Seventh International Conference on Learning Representations (ICLR). OpenReview.net. paper code\n[c5] Ahmed Saleh, Tilman Beck, Lukas Galke, Ansgar Scherp (2018). Performance of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents. In Proceedings of the International Conference on Asian Digital Libraries (ICADL). paper\n[c4] Lukas Galke, Florian Mai, Iacopo Vagliano, Ansgar Scherp (2018). Multi-Modal Adversarial Autoencoders for Recommendation of Citations and Subject Labels. In Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization (UMAP). ACM. paper\n[c3] Anne Lauscher, Kai Eckert, Lukas Galke, Ansgar Scherp, Syed Tassen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Philipp Zumstein, Annette Klein (2018). Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph. In Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL). ACM. paper\n[c2] Florian Mai, Lukas Galke, Ansgar Scherp (2018). Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text. In Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL). ACM. paper\n[c1] Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunch, Ansgar Scherp (2017). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation. In Knowledge Capture Conference (K-CAP). ACM. paper"
  },
  {
    "objectID": "publications.html#workshop-papers",
    "href": "publications.html#workshop-papers",
    "title": "Publications",
    "section": "Workshop papers",
    "text": "Workshop papers\n\n[w9] Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp (2023). GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. GenBench Workshop @ EMNLP 2023. paper\n[w8] Lukas Galke, Yoav Ram, Limor Raviv (2022). Emergent communication for understanding human language evolution: What’s missing?. Emergent Communication workshop at Tenth International Conference on Learning Representations (ICLR 2022). OpenReview.net. paper\n[w7] Lukas Galke, Eva Seidlmayer, Gavin Lüdemann, Lisa Langnickel, Tetyana Melnychuk, Konrad U Förstner, Klaus Tochtermann, Carsten Schultz (2021). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics. In 2021 IEEE International Conference on Big Data (Big Data). IEEE. paper\n[w6] Eva Seidlmayer, Jakob Voß, Tatyana Melnychuk, Lukas Galke, Klaus Tochtermann, Carsten Schultz, Konrad U. Förstner (2020). ORCID for Wikidata — Data enrichment for scientometric applications, Wikidata workshop @ ISWC 2020. paper\n[w5] Eva Seidlmayer, Lukas Galke, Tatyana Melnychuk, Carsten Schultz, Klaus Tochtermann, Konrad U. Förstner (2019): Take it Personally — A Python library for enrichment in informetrical applications. Posters&Demos @ SEMANTICS 2019. paper\n[w4] Lukas Galke, Iacopo Vagliano, Ansgar Scherp (2019). Can Graph Neural Networks Go „Online“? An Analysis of Pretraining and Inference. Representation Learning on Graphs and Manifolds workshop @ ICLR 2019. paper\n[w3] Iacopo Vagliano, Lukas Galke, Florian Mai, Ansgar Scherp (2018). Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation. RecSys Challenge workshop @ RecSys’18. paper\n[w2] Lukas Galke, Gunnar Gerstenkorn, Ansgar Scherp (2018). A Case Study of Closed-Domain Response Suggestion with Limited Training Data. Text-based Information Retrieval workshop @ DEXA’18. paper code\n[w1] Lukas Galke, Ahmed Saleh, Ansgar Scherp (2017). Word Embeddings for Practical Information Retrieval. In INFORMATIK 2017. Gesellschaft für Informatik, Bonn. (S. 2155-2167). paper code (&gt;200 stars, &gt;40 forks)"
  },
  {
    "objectID": "publications.html#extended-abstracts",
    "href": "publications.html#extended-abstracts",
    "title": "Publications",
    "section": "(Extended) abstracts",
    "text": "(Extended) abstracts\n\n[a3] Lukas Galke, Yoav Ram, Limor Raviv (2023). What makes a languagae easy to deep-learn? [1-page abstract]. Protolang 8, 2023.\n[a2] Lukas Galke (2022). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability [selected PhD thesis abstract]. IEEE Intelligent Informatics Bulletin, 22(1), 52. complete issue\n[a1] Lukas Galke, Florian Mai, Ansgar Scherp (2019): What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [extended abstract]. INFORMATIK 2019. GI. paper"
  },
  {
    "objectID": "publications.html#thesis",
    "href": "publications.html#thesis",
    "title": "Publications",
    "section": "Thesis",
    "text": "Thesis\n\nLukas Galke (2023). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability. Number 2023/1 in Kiel Computer Science Series. Department of Computer Science, 2023. Dissertation, Faculty of Engineering, Kiel University. pdf"
  },
  {
    "objectID": "publications.html#project-reports",
    "href": "publications.html#project-reports",
    "title": "Publications",
    "section": "Project reports",
    "text": "Project reports\n\n[r3] Iacopo Vagliano, Till Blume, Lukas Galke, Florian Mai, Ahmed Saleh, Alexandros Pournaras, Nikolaos Gkalelis, Damianos Galanopoulos, Vasileios Mezaris, Ilija Šimić, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Andrea Zielinski, Peter Mutschke (2019). Deliverable 3.3: Technologies for MOVING data processing and visualisation v3.0. report\n[r2] Iacopo Vagliano, Mohammad Abdel-Qader, Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Ilija Šimić, Cecilia di Sciascio, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke (2018). Technologies for MOVING data processing and visualisation v2.0. report\n[r1] Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Matthias Schulte-Althoff, Chrysa Collyda, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Peter Hasitschka, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke Thomas Gottron (2017). Technologies for MOVING data processing and visualisation v1.0. report"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "2022–\nImagine you put some artificial neural network agents into a game that can only be solved with communication. How do artificial neural network agents learn to communicate? What communication protocols emerge? What are the parallels to human communication?\n\nWhat makes a language easy to deep-learn? preprint\nMachine learning and the Evolution of Language (two-session workshop at the Joint Conference on Language Evolution)\nEmeCom workshop paper at ICLR"
  },
  {
    "objectID": "projects.html#interacting-AIs",
    "href": "projects.html#interacting-AIs",
    "title": "Projects",
    "section": "",
    "text": "2022–\nImagine you put some artificial neural network agents into a game that can only be solved with communication. How do artificial neural network agents learn to communicate? What communication protocols emerge? What are the parallels to human communication?\n\nWhat makes a language easy to deep-learn? preprint\nMachine learning and the Evolution of Language (two-session workshop at the Joint Conference on Language Evolution)\nEmeCom workshop paper at ICLR"
  },
  {
    "objectID": "projects.html#textclf",
    "href": "projects.html#textclf",
    "title": "Projects",
    "section": "Are We Really Making Progress in Text Classification (textclf)",
    "text": "Are We Really Making Progress in Text Classification (textclf)\n2021–\nAutomatically categorizing text documents is a popular research topic with numerous new approaches being published each year. However, do they really give an advantage over earlier approaches? This question has triggered this line of research and the answer we have is worrisome. For instance, many recently proposed methods such as TextGCN are outperformed by a simple multilayer perceptron on a bag-of-words representation – a decade old technique enhanced with today’s best practices.\n\npreprint of survey paper\nACL 2022 paper\ncode for multi-label classification\ncode for single-label classification\nextra code for single-label classification with different methods"
  },
  {
    "objectID": "projects.html#LGL",
    "href": "projects.html#LGL",
    "title": "Projects",
    "section": "Lifelong Graph Representation Learning (LGL)",
    "text": "Lifelong Graph Representation Learning (LGL)\n2019–\nGraph neural networks have quickly risen to be the standard technique for machine learning on graph-structured data. Yet graph neural networks are usually only applied to static snapshots of graphs, while real-world graphs (social media, publication networks) are continually evolving. Evolving graphs come with challenges that are rarely reflected in the graph representation learning literature, such as dealing with new classes in node classification. We pursue a lifelong learning approach for graph neural networks on evolving graphs and investigate incremental training, out-of-distribution detection, and issues caused by an imbalanced class distribution.\n\nconference paper on out-of-distribution detection in graphs (coming soon)\nNeural Networks journal paper\nIJCNN 2021 conference paper\nICLR 2019 workshop paper\ncode for Neural Networks paper\ncode for IJCNN’23 paper\nrelated project: Q-AKTIV"
  },
  {
    "objectID": "projects.html#phd-project",
    "href": "projects.html#phd-project",
    "title": "Projects",
    "section": "Representation Learning for Texts and Graphs",
    "text": "Representation Learning for Texts and Graphs\n2017-2022\nMy PhD project. A meta-project bringing together word2mat, textclf, aaerec, LGL. phd thesis"
  },
  {
    "objectID": "projects.html#q-aktiv",
    "href": "projects.html#q-aktiv",
    "title": "Projects",
    "section": "Analyzing the Scientific, Economic and Societal Impact of Research Activities and Research Networks (Q-AKTIV)",
    "text": "Analyzing the Scientific, Economic and Societal Impact of Research Activities and Research Networks (Q-AKTIV)\n2019–2022, funded by the Federal Ministry of Education and Research (BMBF)\nThe aim of Q-AKTIV is to improve the methods for forecasting dynamics and interactions between research, technology development, and innovation. The network analysis methods will be based on recent developments in Deep Learning. In addition to the emergence of new knowledge areas and networks, we focus on the convergence processes of established sectors. The development and evaluation of the new methods initially takes place in the field of life sciences, which is characterized by high marked dynamics. The additional application in economics enables a systematic comparison of the dynamics between the disciplines of science. The new methods will be used to predict the impact of existing research and network structures on the dynamics of knowledge and technologies as well as the future relevance of topics and actors. The result of Q-AKTIV is an implemented and evaluated instrument for the strategic analysis and prognosis of the dynamics in science and innovation. This complements today’s primarily qualitative approaches to early strategic planning and increases the decision-making ability of research institutions, policy makers, and industries. In addition to the analysis of dynamics, also valuable indicators for R & D performance measurement can be derived, e.g., the registration of patents based on scientific publications, the economic development of the companies involved, as well as the outreach of research activities. The practice partner brings in the necessary experience in the field of business valuation and strategy development and ensures a practical testing of the toolkit.\n\none more journal papers currently under review\nTEM paper evaluated the toolkit\nproject website\nCOVID-19++ dataset\ndataset paper.\nconference paper on learning concept representations\nfollow-up work building on concept representation learning methods\nPython package for learning concept representations and analyzing network dynamics\nworkshop paper on data enrichment 2\nworkshop paper on data enrichment 1\ntools for harvesting raw data\ninterview about open science tools for digital collaboration (en)\ninterview about open science tools for digital collaboration (de)\njournal paper (German)\npress item (German)\npress release (ZB MED, German)\nfunding announcement (German)\nfinal project report (German)"
  },
  {
    "objectID": "projects.html#word2mat",
    "href": "projects.html#word2mat",
    "title": "Projects",
    "section": "Word Matrices for Text Representation Learning (word2mat)",
    "text": "Word Matrices for Text Representation Learning (word2mat)\n2017-2022\nThe idea of this project was to embed each word as a matrix as an alternative to vectors used in word embeddings. By using matrix embeddings instead of vector embeddings, we can use matrix multiplication as a composition function to form a representation for phrases and sentences. While word matrices alone did not exceed the performance of word vectors, a combination of word matrices and word vectors turned out to be beneficial. Later, we showed that pre-trained language models can be distilled into such a purely embedding-based model, giving benefits in efficiency while keeping reasonable accuracy.\n\nICLR paper on word matrices\nextended abstract in the best-of data science track at the INFORMATIK 2019\npaper on distilling the knowledge from a pre-trained language model into matrix embedding models\ncode"
  },
  {
    "objectID": "projects.html#aaerec",
    "href": "projects.html#aaerec",
    "title": "Projects",
    "section": "Autoencoders for Document-based Recommendations (aaerec)",
    "text": "Autoencoders for Document-based Recommendations (aaerec)\n2017–2022\nThe aim of this project was to build a document-level citation recommendation system that could, for example, make users aware of missing references. A specialty of this project compared to other recommender systems is that we do not use any a user data or a user profile but only operate on the contents of the current draft. The main research question was whether models could be enhanced by using textual side information, such as the title of the draft, which we confirmed for a wide range of autoencoder-based recommendation models. Interestingly, we found that the choice of the best model depends on the semantics of item co-occurrence. When item co-occurrence implies relatedness (as in citations), looking at other items is far more useful than looking at the text. In contrast, when item co-occurrence implies diversity, such as in subject labels from professional subject indexers, the text is more useful.\n\njournal paper on citation recommendation\nconference paper on citation recommendation\nRecSys challenge paper on music recommendation\ncode"
  },
  {
    "objectID": "projects.html#loc-db",
    "href": "projects.html#loc-db",
    "title": "Projects",
    "section": "Linked Open Citation Database (LOC-DB)",
    "text": "Linked Open Citation Database (LOC-DB)\n2017–2020, funded by Deutsche Forschungsgemeinschaft (DFG)\nThe LOC-DB project will develop ready-to-use tools and processes based on the linked-data technology that make it possible for a single library to meaningfully contribute to an open, distributed infrastructure for the cataloguing of citations. The project aims to prove that, by widely automating cataloguing processes, it is possible to add a substantial benefit to academic search tools by regularly capturing citation relations. These data will be made available in the semantic web to make future reuse possible. Moreover, we document effort, number and quality of the data in a well-founded cost-benefit analysis. The project will use well-known methods of information extraction and adapt them to work for arbitrary layouts of reference lists in electronic and print media. The obtained raw data will be aligned and linked with existing metadata sources. Moreover, it will be shown how these data can be integrated in library catalogues. The system will be deployable to use productively by a single library, but in principle it will also be scalable for using it in a network.\n\nconference paper on citation recommendation\nmain paper on the project as a whole\ndemo of the LOC-DB project outcome\ncollection of code for the LOC-DB project\nSecond Linked Open Citation Database (LOC-DB) workshop\nFirst Linked Open Citation Database (LOC-DB) Workshop\nproject website"
  },
  {
    "objectID": "projects.html#vec4ir",
    "href": "projects.html#vec4ir",
    "title": "Projects",
    "section": "Word Embeddings for Information Retrieval (vec4ir)",
    "text": "Word Embeddings for Information Retrieval (vec4ir)\n2016–2017\nThe key idea was to use word embeddings for similarity scoring in information retrieval. The two main take-aways:\n\nIt is important to retain the crisp matching operation (before similarity scoring), even when using word embeddings.\nA combination of classic information retrieval method TF-IDF and word embeddings led to the best results.\n\n\npaper\nMSc thesis\ncode (&gt;200 stars, &gt;40 forks)"
  },
  {
    "objectID": "projects.html#training-towards-a-society-of-data-savvy-information-professionals-to-enable-open-leadership-innovation-moving",
    "href": "projects.html#training-towards-a-society-of-data-savvy-information-professionals-to-enable-open-leadership-innovation-moving",
    "title": "Projects",
    "section": "TraininG towards a society of data-saVvy inforMation prOfessionals to enable open leadership INnovation (MOVING)",
    "text": "TraininG towards a society of data-saVvy inforMation prOfessionals to enable open leadership INnovation (MOVING)\n2016–2019, EU funding, Grant Agreement Number 693092\nI engaged in this EU Horizon 2020 project as a student worker between 2016 and 2017, leading to contributions to the deliverables 3.1, 3.2 and 3.3, as well as various conference and workshop papers:\n\nDeliverable 3.3\nDeliverable 3.2\nDeliverable 3.1\nICADL 2018 paper on information retrieval on titles vs. full-text\nDEXA 2018 workshop paper on response suggestion\ncode for response suggestion"
  },
  {
    "objectID": "projects.html#xlmc",
    "href": "projects.html#xlmc",
    "title": "Projects",
    "section": "Extreme Multi-label Text Classification (Quadflor)",
    "text": "Extreme Multi-label Text Classification (Quadflor)\n2015–2018\nThis project originated from my Master’s project (2015–2016), where we developed a pipeline for extreme (=many possible classes) multi-label text classification. We found that a multi-layer perceptron beats the state-of-the-art kNN approach by more than 30%. Moreover, we compared using either the full-text or only the title of a research paper as a basis for classification. The result was that the full-text is only marginally better than the title. My team member Florian Mai investigated the trade-off between full-text and title in his Master’s thesis, finding that the increased availability of title data compensates for increased information in full-text articles.\n\ncode (bought by ZBW for production usage)\nK-CAP 2017 paper (outcome of the Master’s project)\nJCDL 2018 paper (outcome of Florian’s Master’s thesis)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "[t29] Lukas Galke (2023, July 20). What makes a language easy to deep-learn? [invited talk]. Colloquium Cognitive Systems, University of Ulm, Germany.\n[t28] Lukas Galke (2023, July 10). Lifelong Learning and Out-of-distribution Detection on Evolving Graphs [invited talk], VU AI Colloquium, Amsterdam, The Netherlands.\n[t25] Lukas Galke (2023, May 16). What makes a language easy to deep-learn? [invited talk]. Computational Linguistics Seminar, University of Amsterdam, The Netherlands."
  },
  {
    "objectID": "talks.html#invited-talks-in-colloquia-and-seminars",
    "href": "talks.html#invited-talks-in-colloquia-and-seminars",
    "title": "Talks",
    "section": "",
    "text": "[t29] Lukas Galke (2023, July 20). What makes a language easy to deep-learn? [invited talk]. Colloquium Cognitive Systems, University of Ulm, Germany.\n[t28] Lukas Galke (2023, July 10). Lifelong Learning and Out-of-distribution Detection on Evolving Graphs [invited talk], VU AI Colloquium, Amsterdam, The Netherlands.\n[t25] Lukas Galke (2023, May 16). What makes a language easy to deep-learn? [invited talk]. Computational Linguistics Seminar, University of Amsterdam, The Netherlands."
  },
  {
    "objectID": "talks.html#informal-talks",
    "href": "talks.html#informal-talks",
    "title": "Talks",
    "section": "Informal talks",
    "text": "Informal talks\n\n[t27] Lukas Galke (2023, July 5). Lifelong Neural Communication [informal talk]. Data Science, Hamburg University, Germany.\n[t26] Lukas Galke (2023, May 17). Uncovering Patterns in Medical Literature through Lifelong Automated Categorization and Research Dynamics Analysis with Machine Learning [informal talk]. KIK AI Coffee & Learning meeting, Amsterdam University Medical Centre, The Netherlands.\n[t24] Lukas Galke (2023, February 27). What makes a language easy to deep learn? [informal talk]. Language Evolution and Emergence meeting, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands.\n[t23] Lukas Galke (2022, October 20). Structure in Language Learning Systems [informal talk]. TALEP Marseille, France.\n[t13] Lukas Galke (2020, June 23). Scaling Up Graph Neural Networks [literature review talk], Graph Neural Networks Reading Group, online."
  },
  {
    "objectID": "talks.html#conference-and-workshop-presentations",
    "href": "talks.html#conference-and-workshop-presentations",
    "title": "Talks",
    "section": "Conference and workshop presentations",
    "text": "Conference and workshop presentations\n\n[t30] Lukas Galke (2023, September 28). What makes a language easy to deep-learn? Protolang 8, Rome, Italy. [upcoming talk]\n[t22] Lukas Galke (2022, September 5). Machine Learning for Language Evolution – What’s missing?. Machine Learning and the Evolution of Language (ml4evolang) workshop at JCOLE’22, Japan/online.\n[t19] Lukas Galke (2022, July 19). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings [paper presentation (oral)]. World Congress on Computational Intelligence, Padua, Italy.\n[t18] Lukas Galke (2022, June 1). Emergent Communication for Understanding Human Language Evolution: What’s missing? [paper presentation (poster)]. IMPRS conference, Nijmegen, The Netherlands.\n[t17] Lukas Galke (2022, May 24). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP [paper presentation (oral)]. ACL 2022. video\n[t16] Lukas Galke (2022, April 29). Emergent Communication for Understanding Human Language Evolution [paper presentation (oral) and discussion session]. EmeCom workshop at ICLR’22, online.\n[t15] Lukas Galke (2021, December 15). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics [paper presentation (oral)]. 2021 IEEE International Conference on Big Data (Big Data), online.\n[t14] Lukas Galke (2021, July 18). Lifelong learning of graph neural networks for open-world node classification [paper presentation (oral)]. International Joint Conference on Neural Networks, online.\n[t12] Lukas Galke (2019, September 26). Inductive Learning of Concept Representations from Library-Scale Corpora with Graph Convolution [paper presentation (oral)], INFORMATIK 2019, Kassel, Germany.\n[t11] Lukas Galke (2019, September 26). What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [paper presentation (oral)]. INFORMATIK 2019, Kassel, Germany\n[t10] Lukas Galke, Florian Mai (2019, May 8). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.\n[t9] Lukas Galke (2019, May 7). Can Graph Neural Networks Go Online? An Analysis of Pretraining and Inference [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.\n[t8] Anne Lauscher, Lukas Galke, Syed Tahseen Raza Rizvi (2018, November 6). LOC-DB Evaluation: criteria and preliminary results. Second Linked Open Citation Database (LOC-DB) workshop. [project presentation]\n[t7] Lukas Galke (2018, July 10). Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels [paper presentation (oral)]. Conference on User Modeling, Adaptation and Personalization, Singapore, Singapore.\n[t6] Lukas Galke (2017, December 6). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation [paper presentation (oral)], Knowledge Capture Conference, Austin, Texas.\n[t5] Kai Eckert, Anne Lauscher, Lukas Galke (2017, November 7). LOC-DB Konzepte. First Linked Open Citation Database (LOC-DB) Workshop. [project presentation]\n[t4] Lukas Galke (2017, September 28). Reranking-based Recommender Systems with Deep Learning [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.\n[t3] Lukas Galke (2017, September 28). Word Embeddings for Practical Information Retrieval [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.\n[t2] Lukas Galke (2017, May 5). Embedded Retrieval: Word Embeddings for Practical Information Retrieval [M.Sc. thesis presentation]. Second DyESE workshop, Kiel, Germany.\n[t1] Lukas Galke (2016, September 16). Information Retrieval on Sparse Data [concept presentation]. First DyESE workshop, Oslo, Norway."
  },
  {
    "objectID": "talks.html#other",
    "href": "talks.html#other",
    "title": "Talks",
    "section": "Other",
    "text": "Other\n\n[t21] Lukas Galke (2022, August 28). Representation Learning for Texts and Graphs [viva]. Department of Computer Science, Kiel University, Germany.\n[t20] Lukas Galke (2022, August 18). Language Technology [gave a 2x45min workshop]. Regiodag 2022 Probusclub Nijmegen e.o., Nijmegen, The Netherlands."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Galke",
    "section": "",
    "text": "I’m Lukas, a postdoctoral researcher at the Max Planck Institute for Psycholinguistics. I’m most passionate about natural language processing and lifelong machine learning, while often engaging in interdisciplinary projects. Currently, I’m focusing on how deep neural networks learn to communicate and the parallels to human communication."
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Lukas Galke",
    "section": "Selected publications",
    "text": "Selected publications\n\n[j7] Lukas Galke, Yoav Ram, Limor Raviv (in revision). What makes a language easy to deep-learn?\n[j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176.\n[c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics.\n\nfind more publications here"
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "Lukas Galke",
    "section": "Recent news",
    "text": "Recent news\n\nDec 2023: received best paper award from GenBench workshop, EMNLP 2023\nJul 2023: received outstanding reviewer award from ACL 2023\nJul 2023: gave a talk in the Cognitive Systems Colloquium, University of Ulm [t29]\nJul 2023: gave a talk in the AI Colloquium of Vrije Universiteit Amsterdam [t28]\nMay 2023: paper on lifelong graph learning published in Neural Networks [j3]\nMay 2023: gave a talk in the KIK AI Coffee and Learning meeting at Amsterdam University Medical Centre [t26]\nMay 2023: gave a talk in the Computational Linguistics Seminar at University of Amsterdam [t25]"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Lukas Galke",
    "section": "Links",
    "text": "Links\n\nORC ID: 0000-0001-6124-1092\nDBLP profile\nSemantic Scholar profile\nGoogle Scholar profile\nGithub profile\nMastodon account"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Lukas Galke",
    "section": "Contact",
    "text": "Contact\nTo get in touch, just drop me an e-mail via hi@lpag.de."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am the kind of person that is motivated by challenges.\nI practice Yoga.\nI like to go surfing and stand-up paddling.\nI like reading. Currently reading: Fourth Wing by Rebecca Yarros. Current book recommendations: The Light Pirate by Lily Brooks-Dalton, Station 11 by Emily St. John Mandel.\nI like to play chess.\nI like [drone] photography.\nI play Saxophone since childhood, but only at Christmas these days.\nI am learning to play Ukulele (on hold).\nI have an inner voice – not everyone has one."
  },
  {
    "objectID": "about.html#some-non-professional-things-about-myself",
    "href": "about.html#some-non-professional-things-about-myself",
    "title": "About",
    "section": "",
    "text": "I am the kind of person that is motivated by challenges.\nI practice Yoga.\nI like to go surfing and stand-up paddling.\nI like reading. Currently reading: Fourth Wing by Rebecca Yarros. Current book recommendations: The Light Pirate by Lily Brooks-Dalton, Station 11 by Emily St. John Mandel.\nI like to play chess.\nI like [drone] photography.\nI play Saxophone since childhood, but only at Christmas these days.\nI am learning to play Ukulele (on hold).\nI have an inner voice – not everyone has one."
  },
  {
    "objectID": "about.html#some-photos",
    "href": "about.html#some-photos",
    "title": "About",
    "section": "Some photos",
    "text": "Some photos\n\n\n\nBeach Mood, Voropør, Denmark, June 2020\n\n\n\n\n\nHiking in Blaubeuren, Germany, July 2023\n\n\n\n\n\nSecret Island, Sweden, June 2022\n\n\n\n\n\nYoga on a Rock, Sweden, July 2022\n\n\n\n\n\nNorth Sea meets Baltic Sea, Skagen, Denmark, August 2021\n\n\n\n\n\nKruemel Robusto in Grena, Denmark, August 2021\n\n\n\n\n\nDay trip to Venice, Italy, July 2022\n\n\n\n\n\nSit-down Paddling in Germany, June 2021"
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "About",
    "section": "About this website",
    "text": "About this website\nWhenever I need to look up something about myself, I look here first. If I don’t find it, I add it. I use this website as a playground for web technology and design (so expect relatively frequent changes)."
  }
]
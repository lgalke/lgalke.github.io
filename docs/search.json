[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Lukas Galke",
    "section": "",
    "text": "[t22] Lukas Galke (2023, May 17). Uncovering Patterns in Medical Literature through Lifelong Automated Categorization and Research Dynamics Analysis with Machine Learning [invited talk]. KIK AI Coffee & Learning meeting, Amsterdam University Medical Centre, The Netherlands.\n[t21] Lukas Galke (2023, May 16). What makes a language easy to deep learn? [invited talk]. Computational Linguistics Seminar, University of Amsterdam, The Netherlands.\n[t20] Lukas Galke (2023, February 27). What makes a language easy to deep learn? [paper presentation]. Language Evolution and Emergence meeting, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands.\n[t19] Lukas Galke (2022, October 20). Structure in Language Learning Systems [informal talk]. TALEP Marseille, online.\n[t18] Lukas Galke (2022, September 5). Machine Learning for Language Evolution – What’s missing? [paper presentation (oral)]. Machine Learning and the Evolution of Language (ml4evolang) workshop at JCOLE’22, Japan/online.\n[t17] Lukas Galke (2022, August 28). Representation Learning for Texts and Graphs [viva]. Department of Computer Science, Kiel University, Germany.\n[t16] Lukas Galke (2022, August 18). Language Technology [workshop]. Regiodag 2022 Probusclubs Nijmegen e.o., Nijmegen, The Netherlands.\n[t15] Lukas Galke (2022, July 19). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings [paper presentation (oral)]. World Congress on Computational Intelligence, Padua, Italy.\n[t14] Lukas Galke (2022, June 1). Emergent Communication for Understanding Human Language Evolution: What’s missing? [paper presentation (poster)]. IMPRS conference, Nijmegen, The Netherlands.\n[t13] Lukas Galke (2022, May 24). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP [paper presentation (oral)]. ACL 2022. https://doi.org/10.48448/09t2-ck98\n[t12] Lukas Galke (2022, April 29). Emergent Communication for Understanding Human Language Evolution [paper presentation (oral) and discussion session]. EmeCom workshop at ICLR’22, online.\n[t11] Lukas Galke (2021, December 15). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics [paper presentation (oral)]. 2021 IEEE International Conference on Big Data (Big Data), online.\n[t10] Lukas Galke (2021, July 18). Lifelong learning of graph neural networks for open-world node classification [paper presentation (oral)]. International Joint Conference on Neural Networks, online.\n[t9] Lukas Galke (2020, June 23). Scaling Up Graph Neural Networks [paper presentation (oral)], Graph Neural Network Reading Group, online.\n[t8] Lukas Galke (2019, September 26). Inductive Learning of Concept Representations from Library-Scale Corpora with Graph Convolution [paper presentation (oral)], INFORMATIK 2019, Kassel, Germany.\n[t7] Lukas Galke (2019, September 26). What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [paper presentation (oral)]. INFORMATIK 2019, Kassel, Germany\n[t6] Lukas Galke, Florian Mai (2019, May 8). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.\n[t5] Lukas Galke (2019, May 7). Can Graph Neural Networks Go Online? An Analysis of Pretraining and Inference [paper presentation (poster)]. International Conference on Learning Representations, New Orleans, Lousiana.\n[t4] Lukas Galke (2018, July 10). Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels [paper presentation (oral)]. Conference on User Modeling, Adaptation and Personalization, Singapore, Singapore.\n[t3] Lukas Galke (2017, December 6). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation [paper presentation (oral)], Knowledge Capture Conference, Austin, Texas.\n[t2] Lukas Galke (2017, September 28). Reranking-based Recommender Systems with Deep Learning [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany.\n[t1] Lukas Galke (2017, September 28). Word Embeddings for Practical Information Retrieval [paper presentation (oral)]. INFORMATIK 2017, Chemnitz, Germany."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi there!",
    "section": "",
    "text": "I’m Lukas, a postdoc in the LEADS group led by Limor Raviv at the Max Planck Institute for Psycholinguistics. I’m most passionate about natural language processing and continual learning. Currently, I’m focusing on how deep nets learn to communicate and how that relates to human language learning.\nI did my PhD on text and graph representation learning with Ansgar Scherp at Kiel University, where I also obtained my Master’s degree in Computer Science. I’ve worked on text classification, information retrieval, text representation learning, graph representation learning, and lifelong (or continual) learning.\nBesides research, I enjoy surfing, vanlife, and other sorts of outdoor adventures. I like playing chess, doing yoga, and automatizing stuff (of course!)."
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Hi there!",
    "section": "Selected publications",
    "text": "Selected publications\n\n[j5] Lukas Galke, Yoav Ram, Limor Raviv (under review). What makes a language easy to deep-learn?\n[j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176. https://doi.org/10.1016/j.neunet.2023.04.022\n[c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics.\n\nmore publications"
  },
  {
    "objectID": "index.html#current-project",
    "href": "index.html#current-project",
    "title": "Hi there!",
    "section": "Current project",
    "text": "Current project\nNeural networks are still far behind human capabilities in generalizing from few examples and continual learning. I want to fix that.\nMy current research investigates how neural network agents learn to communicate. Imagine you put some (artificial, of course) neural network agents into a game that can only be solved with communication. How do communication protocols emerge? How similar are those to emergent languages of humans in the same setting? What can we learn from humans to improve our models?\nThis relates to the field of emergent communication. My first baby-step into the field is a workshop paper that summarizes recent progress of the emergent communication field and contrasts it with linguistic phenomena in humans. I co-organized a workshop on machine learning for language evolution research at the Joint Conference for Language Evolution."
  },
  {
    "objectID": "index.html#past-projects",
    "href": "index.html#past-projects",
    "title": "Hi there!",
    "section": "Past projects",
    "text": "Past projects\n\nQ-AKTIV - Quantitative Analysis of the Dynamics of the Scientific, Economic, and Social Impact of Research Activities and Networks\nLinked Open Citation Database"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Hi there!",
    "section": "Contact",
    "text": "Contact\nTo get in touch, just drop me an e-mail via lukas.galke@mpi.nl or reach out via Mastodon."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Lukas Galke",
    "section": "",
    "text": "[j7] Lukas Galke, Yoav Ram, Limor Raviv (in prep). What makes a language easy to deep-learn?\n[j6] Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (in prep). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19.\n[j5] Lukas Galke, Andor Diera, Bao Xin Lin, Bhakti Khera, Tim Meuser, Tushar Singal, Fabian Karl, Ansgar Scherp (under review). Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single-label and Multi-label Text Classification.\n[j4] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad Förstner, Klaus Tochtermann, Carsten Schultz (in revision). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence.\n[j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. Neural Networks 164, 156-176. https://doi.org/10.1016/j.neunet.2023.04.022\n[j2] Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. Inf Retrieval J 25, 269–305. https://doi.org/10.1007/s10791-022-09408-9\n[j1] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement [translated: Early-detection of scientic convergence in university management]. Hochschulmanagement 16(1).\n\n\n\n\n\n[c11] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (in press). Open-World Lifelong Graph Learning. In International Joint Conference on Neural Networks (IJCNN). IEEE.\n[c10] Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand Nölscher, Angelina Sonderecker, Ansgar Scherp (2022). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE. https://doi.org/10.1109/IJCNN55064.2022.9892144\n[c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.279\n[c8] Lukas Galke, Benedikt Franke, Tobias Zielke, Ansgar Scherp (2021). Lifelong Learning of Graph Neural Networks for Open-World Node Classication. In 2021 International Joint Conference on Neural Networks (IJCNN). IEEE. https://doi.org/10.1109/IJCNN52387.2021.9533412\n[c7] Lukas Galke, Tetyana Melnychuk, Eva Seidlmayer, Steffen Trog, Konrad U. Förster, Carsten Schultz, Klaus Tochtermann (2019). Inductive Learning of Concept Representations from Library-Scale Bibliographic Corpora. In: INFORMATIK 2019. GI. https://doi.org/10.18420/inf2019_26\n[c6] Florian Mai, Lukas Galke, Ansgar Scherp (2019). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model. In International Conference on Learning Representations (ICLR). OpenReview.net.\n[c5] Ahmed Saleh, Tilman Beck, Lukas Galke, Ansgar Scherp (2018). Performance of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents. In International Conference on Asian Digital Libraries (ICADL).\n[c4] Lukas Galke, Florian Mai, Iacopo Vagliano (2018). Multi-Modal Adversarial Autoencoders for Recommendation of Citations and Subject Labels. In Conference on User Modeling, Adaptation and Personalization (UMAP). ACM. https://doi.org/10.1145/3209219.3209236\n[c3] Anne Lauscher, Kai Eckert, Lukas Galke, Ansgar Scherp, Syed Tassen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Philipp Zumstein, Annette Klein (2018). Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph. In Joint Conference on Digital Libraries (JCDL). ACM.\n[c2] Florian Mai, Lukas Galke, Ansgar Scherp (2018). Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text. In Joint Conference on Digital Libraries (JCDL). ACM. https://doi.org/10.1145/3197026.3197039\n[c1] Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunch, Ansgar Scherp (2017). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation. In Knowledge Capture Conference (K-CAP). ACM. https://doi.org/10.1145/3148011.3148039\n\n\n\n\n\n[w8] Lukas Galke, Yoav Ram, Limor Raviv (2022). Emergent communication for understanding human language evolution: What’s missing?. Emergent Communication workshop at The Tenth International Conference on Learning Representations (ICLR 2022).OpenReview.net. https://openreview.net/forum?id=rqUGZQ-0XZ5\n[w7] Lukas Galke, Eva Seidlmayer, Gavin Lüdemann, Lisa Langnickel, Tetyana Melnychuk, Konrad U Förstner, Klaus Tochtermann, Carsten Schultz (2021). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics. In 2021 IEEE International Conference on Big Data (Big Data). IEEE.\n[w6] Eva Seidlmayer, Jakob Voß, Tatyana Melnychuk, Lukas Galke, Klaus Tochtermann, Carsten Schultz, Konrad U. Förstner: ORCID for Wikidata — Data enrichment for scientometric applications, Wikidata workshop @ ISWC 2020.\n[w5] Eva Seidlmayer, Lukas Galke, Tatyana Melnychuk, Carsten Schultz, Klaus Tochtermann, Konrad U. Förstner (2019): Take it Personally — A Python library for enrichment in informetrical applications. Posters&Demos @ SEMANTICS 2019.\n[w4] Lukas Galke, Iacopo Vagliano, Ansgar Scherp (2019). Can Graph Neural Networks Go „Online“? An Analysis of Pretraining and Inference. Representation Learning on Graphs and Manifolds workshop @ ICLR 2019.\n[w3] Iacopo Vagliano, Lukas Galke, Florian Mai, Ansgar Scherp (2018). Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation. RecSys Challenge workshop @ RecSys’18.\n[w2] Lukas Galke, Gunnar Gerstenkorn, Ansgar Scherp (2018). A Case Study of Closed-Domain Response Suggestion with Limited Training Data. Text-based Information Retrieval workshop @ DEXA’18.\n[w1] Lukas Galke, Ahmed Saleh, Ansgar Scherp (2017). Word Embeddings for Practical Information Retrieval. In INFORMATIK 2017. Gesellschaft für Informatik, Bonn. (S. 2155-2167). https://doi.org/10.18420/in2017_215\n\n\n\n\n\n[a2] Lukas Galke (2022). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability [selected PhD thesis abstract]. IEEE Intelligent Informatics Bulletin, 22(1), 52. https://www.comp.hkbu.edu.hk/~cib/2022/IIB2022_Final.pdf\n[a1] Lukas Galke, Florian Mai, Ansgar Scherp (2019): What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [extended abstract]. INFORMATIK 2019. GI.\n\n\n\n\n\nLukas Galke (2023). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability. Number 2023/1 in Kiel Computer Science Series. Department of Computer Science, 2023. Dissertation, Faculty of Engineering, Kiel University. https://doi.org/10.21941/kcss/2023/1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]
---
title: "Hi there!"
---

<img width="256" height="256" resize=true src="images/2022-05-Lukas-Galke.jpg"></img>


I'm a postdoc in the LEADS group led by [Limor
Raviv](https://www.limorravivevolang.com) at the [Max Planck Institute for
Psycholinguistics](https://mpi.nl).  I'm most passionate about natural language
processing and continual learning. Right now, I'm focusing on how deep
nets learn to communicate and how that relates to human language learning.

I did my PhD on text and graph representation learning with [Ansgar
Scherp](http://ansgarscherp.net) at [Kiel
University](https://www.uni-kiel.de/en/), where I also obtained my Masterâ€™s
degree in Computer Science.  I've worked on text classification, information
retrieval, self-supervised text representation learning, multimodal autoencoders,
and lifelong graph learning.

Besides research, I enjoy surfing, vanlife, and other sorts of outdoor
adventures. I also like playing chess, doing yoga, and making music. I choose
vim over emacs and actively prevent myself from spending more time with
vimscript than the result would save me. 

## Selected publications

- Galke & Scherp: [Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP](https://aclanthology.org/2022.acl-long.279/), ACL 2022.
- Vagliano, Galke, & Scherp: [Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities](https://link.springer.com/article/10.1007/s10791-022-09408-9), *Information Retrieval* 2022.
- Galke, Franke, Zielke, & Scherp: [Lifelong learning of graph neural networks for open-world node classification](https://ieeexplore.ieee.org/abstract/document/9533412), IJCNN 2021.

To see more of my publications, visit my [Google Scholar profile](https://scholar.google.de/citations?user=AHGGdYQAAAAJ&hl=en) or my [DBLP profile](https://dblp.org/pid/200/7830.html).

## Current project 

Neural networks are still far behind human capabilities in generalizing from
few examples and continual learning. I want to fix that.

My current research investigates how neural network agents learn to
communicate. Imagine you put some (artificial, of course) neural network agents
into a game that can only be solved with communication. How do communication
protocols emerge?  How similar are those to emergent languages of humans in the
same setting? What can we learn from humans to improve our models?

This relates very well to the field of emergent communication. My first
baby-step into the field is a [workshop
paper](https://arxiv.org/abs/2204.10590) that summarizes recent progress of the
emergent communication field and contrasts it with linguistic phenomena in
humans. Very recently, I co-organized a [workshop on machine learning for language
evolution research](https://ml4evolang.github.io/) at the [Joint Conference for
Language
Evolution](https://sites.google.com/view/joint-conf-language-evolution/home).


## Past projects

- [Q-AKTIV - Quantitative Analysis of the Dynamics of the Scientific, Economic, and Social Impact of Research Activities and Networks](https://q-aktiv.github.io)
- [Linked Open Citation Database](https://locdb.bib.uni-mannheim.de/blog/en/)

## Contact

I'm usually open to collaborate as long as it's mildly related to what I'm
doing. To get in touch, just drop me an e-mail via
[lukas.galke@mpi.nl](mailto:lukas.galke@mpi.nl?subject=First contact). Other
than that, feel free to follow and/or contact me on
[Twitter](https://twitter.com/LukasGalke).
 



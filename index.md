---
title: "Lukas Paul Achatius Galke"
---


I'm Lukas, a postdoc in the LEADS group led by [Limor
Raviv](https://www.limorravivevolang.com) at the [Max Planck Institute for
Psycholinguistics](https://mpi.nl).  I'm most passionate about natural language
processing and continual learning. Currently, I'm focusing on how deep
nets learn to communicate and how that relates to human language learning.

I did my PhD on text and graph representation learning with [Ansgar
Scherp](http://ansgarscherp.net) at [Kiel
University](https://www.uni-kiel.de/en/), where I also obtained my Master’s
degree in Computer Science. I've worked on text classification, information
retrieval, text representation learning, graph representation learning, and
lifelong (or continual) learning.

Besides research, I enjoy surfing, vanlife, and other sorts of outdoor
adventures. I like playing chess, doing yoga, and automatizing stuff (of course!). 

## Selected publications

- [j7] Lukas Galke, Yoav Ram, Limor Raviv (in prep). [What makes a language easy to deep-learn?](https://arxiv.org/abs/2302.12239)
- [j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). [Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes](https://authors.elsevier.com/a/1h1SX3BBjKnulZ). *Neural Networks* 164, 156-176. [https://doi.org/10.1016/j.neunet.2023.04.022](https://doi.org/10.1016/j.neunet.2023.04.022)
- [c9] Lukas Galke, Ansgar Scherp (2022). [Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP](https://doi.org/10.18653/v1/2022.acl-long.279). In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics.

[more publications](./publications.html)

## Current project 

My current research investigates how neural network agents learn to
communicate. Imagine you put some (artificial, of course) neural network agents
into a game that can only be solved with communication. How do communication
protocols emerge? How similar are those to emergent languages of humans in the
same setting? What can we learn from humans to improve our models?

## Past projects

- [Q-AKTIV - Quantitative Analysis of the Dynamics of the Scientific, Economic, and Social Impact of Research Activities and Networks](https://q-aktiv.github.io)
- [Linked Open Citation Database](https://locdb.bib.uni-mannheim.de/blog/en/)

## Contact

To get in touch, just drop me an e-mail via
[lukas.galke@mpi.nl](mailto:lukas.galke@mpi.nl?subject=First contact) or  reach out via <a rel="me" href="https://sigmoid.social/@lpag">Mastodon</a>.
 



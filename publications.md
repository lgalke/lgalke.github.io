---
title: Publications
---

## Journal articles

- [j7] Lukas Galke, Limor Raviv (2025). Learning and communication pressures in neural networks: Lessons from emergent communication. *Language Development Research* 5(1). [paper](https://doi.org/10.34842/3vr5-5r49) [preprint](https://arxiv.org/abs/2403.14427)
- [j6] Lukas Galke, Yoav Ram, Limor Raviv (2024). Deep neural networks and humans both benefit from compositional language structure. *Nature Communications* 15:10816 [paper](https://rdcu.be/d5f2e) [code](https://github.com/lgalke/easy2deeplearn) [data](https://doi.org/10.5281/zenodo.14205452)
- [j5] Eva Seidlmayer, Tetyana Melnychuk, Lukas Galke, Lisa Kühnel, Klaus Tochtermann, Carsten Schultz, Konrad Förstner (2024). Research Topic Displacement and the Lack of Interdisciplinarity: Lessons from the Scientific Response to COVID-19. *Scientometrics*. [paper](https://doi.org/10.1007/s11192-024-05132-x)
- [j4] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer; Stefanie Bröring, Konrad U. Förstner, Klaus Tochtermann, Carsten Schultz (2023). Development of Similarity Measures from Graph-Structured Bibliographic Metadata: An Application to Identify Scientific Convergence. *IEEE Transactions on Engineering Management*. [paper](https://doi.org/10.1109/TEM.2023.3308008)
- [j3] Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, Ansgar Scherp (2023). Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. *Neural Networks* 164, 156-176. [paper](https://pure.mpg.de/rest/items/item_3368482_4/component/file_3510107/content) [code](https://github.com/lgalke/lifelong-learning)
- [j2] Iacopo Vagliano, Lukas Galke, Ansgar Scherp (2022). Recommendations for item set completion: on the semantics of item co-occurrence with data sparsity, input size, and input modalities. *Inf Retrieval J* 25, 269–305. [paper](https://doi.org/10.1007/s10791-022-09408-9) [code](https://github.com/lgalke/aae-recommender)
- [j1] Tetyana Melnychuk, Lukas Galke, Eva Seidlmayer, Konrad Ulrich Förster, Klaus Tochtermann, Carsten Schultz (2021). Früherkennung wissenschaftlicher Konvergenz im Hochschulmanagement [translated: Early-detection of scientic convergence in university management]. *Hochschulmanagement* 16(1). [complete issue](https://www.universitaetsverlagwebler.de/_files/ugd/7bac3c_24fe9adc2e3740178ad5ba98f66d1931.pdf)

## Conference papers

- [c19] Richard Šléher, William Brach, Tibor Sloboda, Kristián Košťál, and Lukas Galke (2025). Guarded Query Routing for Large Language Models. To appear in *ECAI*. [preprint](https://arxiv.org/abs/2505.14524)
- [c18] Marcel Hoffmann, Lukas Galke, and Ansgar Scherp (2025). Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax. To appear in *ECAI*.
- [c17] Andrea Blasi Nuñez, Lukas Galke, and Peter Schneider-Kamp (2025). MLDataForge: Accelerating Large-Scale Dataset Preprocessing and Access for Multimodal Foundation Model Training. To appear in *RaNLP*.
- [c16] Jacob Nielsen, Peter Schneider-Kamp, and Lukas Galke (2025). Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?
*ACL 2025 Findings*. [preprint](https://arxiv.org/abs/2502.11895)
- [c15] Andor Diera, Lukas Galke, and Ansgar Scherp (2025). Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search. *European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2025)*. [preprint](https://arxiv.org/abs/2411.17538) [code](https://github.com/drndr/code_isotropy)
- [c14] Jacob Nielsen, Lukas Galke, and Peter-Schneider Kamp (2025). When are 1.58 bits enough? A Bottom-up Exploration of Quantization-aware Training with Ternary Weights. *17th International Conference on Agents and Artificial Intelligence (ICAART 2025)*. [preprint](https://arxiv.org/abs/2411.05882)
- [c13] Yousef Younes, Lukas Galke, and Ansgar Scherp (2024). RADAr: A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification. *27th European Conference on Artificial Intelligence 2024 (ECAI 2024)*. [paper](https://ebooks.iospress.nl/doi/10.3233/FAIA240661) [code](https://github.com/yousef-younes/RADAr)
- [c12] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2024). POWN: Prototypical Open-world Node Classification.  *Conference on Lifelong Learning Agents (CoLLAs 2024)*. [paper](https://lifelong-ml.cc/Conferences/2024/acceptedpapersandvideos/conf-2024-38) [code](https://github.com/Bobowner/POWN)
- [c11] Marcel Hoffmann, Lukas Galke, Ansgar Scherp (2023). Open-World Lifelong Graph Learning. In *International Joint Conference on Neural Networks (IJCNN 2023)*. IEEE. [paper](https://doi.org/10.1109/IJCNN54540.2023.10191071) [code](https://github.com/Bobowner/Open-World-LGL)
- [c10] Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand Nölscher, Angelina Sonderecker, Ansgar Scherp (2022). General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. In *Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN 2022)*. IEEE. [paper](https://doi.org/10.1109/IJCNN55064.2022.9892144)
- [c9] Lukas Galke, Ansgar Scherp (2022). Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 4038–4051, Dublin, Ireland. Association for Computational Linguistics. [paper](https://doi.org/10.18653/v1/2022.acl-long.279) [code](https://github.com/lgalke/text-clf-baselines)
- [c8] Lukas Galke, Benedikt Franke, Tobias Zielke, Ansgar Scherp (2021). Lifelong Learning of Graph Neural Networks for Open-World Node Classification. In *Proceedings of the 2021 International Joint Conference on Neural Networks (IJCNN 2021)*. IEEE. [paper](https://doi.org/10.1109/IJCNN52387.2021.9533412) [code](https://github.com/lgalke/lifelong-learning)
- [c7] Lukas Galke, Tetyana Melnychuk, Eva Seidlmayer, Steffen Trog, Konrad U. Förster, Carsten Schultz, Klaus Tochtermann (2019). Inductive Learning of Concept Representations from Library-Scale Bibliographic Corpora. *INFORMATIK 2019*. GI. [paper](https://doi.org/10.18420/inf2019_26)
- [c6] Florian Mai, Lukas Galke, Ansgar Scherp (2019). CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model. In *Proceedings of the Seventh International Conference on Learning Representations (ICLR 2019)*. OpenReview.net. [paper](https://openreview.net/pdf?id=H1MgjoR9tQ) [code](https://github.com/florianmai/word2mat)
- [c5] Ahmed Saleh, Tilman Beck, Lukas Galke, Ansgar Scherp (2018). Performance of Ad-Hoc Retrieval Models over Full-Text vs. Titles of Documents. In *Proceedings of the International Conference on Asian Digital Libraries (ICADL 2018)*. [paper](https://doi.org/10.1007/978-3-030-04257-8_30)
- [c4] Lukas Galke, Florian Mai, Iacopo Vagliano, Ansgar Scherp (2018). Multi-Modal Adversarial Autoencoders for Recommendation of Citations and Subject Labels. In *Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization (UMAP 2018)*. ACM. [paper](https://doi.org/10.1145/3209219.3209236)
- [c3] Anne Lauscher, Kai Eckert, Lukas Galke, Ansgar Scherp, Syed Tassen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, Philipp Zumstein, Annette Klein (2018). Linked Open Citation Database: Enabling Libraries to Contribute to an Open and Interconnected Citation Graph. In *Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL 2018)*. ACM. [paper](https://doi.org/10.1145/3197026.3197050)
- [c2] Florian Mai, Lukas Galke, Ansgar Scherp (2018). Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text. In *Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL 2018)*. ACM. [paper](https://doi.org/10.1145/3197026.3197039)
- [c1] Lukas Galke, Florian Mai, Alan Schelten, Dennis Brunch, Ansgar Scherp (2017). Using Titles vs. Full-text as Source for Automated Semantic Document Annotation. In *Knowledge Capture Conference (K-CAP 2017)*. ACM. [paper](https://doi.org/10.1145/3148011.3148039)

## Workshop papers

- [w10] Anh Dang, Limor Raviv, Lukas Galke (2024). Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test. In *Cognitive Modeling and Computational Linguistics Workshop at ACL*. [paper](https://aclanthology.org/2024.cmcl-1.15/)
- [w9] Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp (2023). GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. GenBench Workshop @ EMNLP 2023. [paper](https://aclanthology.org/2023.genbench-1.2/)
- [w8] Lukas Galke, Yoav Ram, Limor Raviv (2022). Emergent communication for understanding human language evolution: What's missing?. Emergent Communication workshop at *Tenth International Conference on Learning Representations (ICLR 2022)*. OpenReview.net. [paper](https://openreview.net/forum?id=rqUGZQ-0XZ5)
- [w7] Lukas Galke, Eva Seidlmayer, Gavin Lüdemann, Lisa Langnickel, Tetyana Melnychuk, Konrad U Förstner, Klaus Tochtermann, Carsten Schultz (2021). COVID-19++: A Citation-Aware Covid-19 Dataset for the Analysis of Research Dynamics. In *2021 IEEE International Conference on Big Data (Big Data)*. IEEE. [paper](https://doi.org/10.1109/BigData52589.2021.9671730)
- [w6] Eva Seidlmayer, Jakob Voß, Tatyana Melnychuk, Lukas Galke, Klaus Tochtermann, Carsten Schultz, Konrad U. Förstner (2020). ORCID for Wikidata — Data enrichment for scientometric applications, Wikidata workshop @ *ISWC 2020*. [paper](https://ceur-ws.org/Vol-2773/paper-09.pdf)
- [w5] Eva Seidlmayer, Lukas Galke, Tatyana Melnychuk, Carsten Schultz, Klaus Tochtermann, Konrad U. Förstner (2019): Take it Personally — A Python library for enrichment in informetrical applications. Posters&Demos @ *SEMANTICS 2019*. [paper](https://ceur-ws.org/Vol-2451/paper-23.pdf)
- [w4] Lukas Galke, Iacopo Vagliano, Ansgar Scherp (2019). Can Graph Neural Networks Go „Online“? An Analysis of Pretraining and Inference. Representation Learning on Graphs and Manifolds workshop @ *ICLR 2019*. [paper](https://rlgm.github.io/papers/21.pdf)
- [w3] Iacopo Vagliano, Lukas Galke, Florian Mai, Ansgar Scherp (2018). Using Adversarial Autoencoders for Multi-Modal Automatic Playlist Continuation. RecSys Challenge workshop @ *RecSys'18*. [paper](https://doi.org/10.1145/3267471.3267476)
- [w2] Lukas Galke, Gunnar Gerstenkorn, Ansgar Scherp (2018). A Case Study of Closed-Domain Response Suggestion with Limited Training Data. Text-based Information Retrieval workshop @ *DEXA'18*. [paper](https://doi.org/10.1007/978-3-319-99133-7_18) [code](https://github.com/lgalke/resuggest)
- [w1] Lukas Galke, Ahmed Saleh, Ansgar Scherp (2017). Word Embeddings for Practical Information Retrieval. In *INFORMATIK 2017*. Gesellschaft für Informatik, Bonn. (S. 2155-2167). [paper](https://doi.org/10.18420/in2017_215) [code (>200 stars, >40 forks)](https://github.com/lgalke/vec4ir)

## (Extended) abstracts

- [a5] Lukas Galke, Yoav Ram, Limor Raviv (2024). Learning Pressures and Inductive Biases in Emergent Communication: Parallels between Humans and Deep Neural Networks. In: *The evolution of language: Proceedings of the 15th international conference (Evolang XV)*.
- [a4] Anh Dang, Limor Raviv, Lukas Galke (2024). Testing the Linguistic Niche Hypothesis in Large Language Models with a Multilingual Wug Test. In: *The evolution of language: Proceedings of the 15th international conference (Evolang XV)*.
- [a3] Lukas Galke, Yoav Ram, Limor Raviv (2023). What makes a language easy to deep-learn? *Protolang 8*, 2023.
- [a2] Lukas Galke (2022). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability [selected PhD thesis abstract]. *IEEE Intelligent Informatics Bulletin*, 22(1), 52. [complete issue](https://www.comp.hkbu.edu.hk/~cib/2022/IIB2022_Final.pdf)
- [a1] Lukas Galke, Florian Mai, Ansgar Scherp (2019): What If We Encoded Words as Matrices and Used Matrix Multiplication as Composition Function [extended abstract]. *INFORMATIK 2019*. GI. [paper](https://doi.org/10.18420/inf2019_47)

## Thesis

- Lukas Galke (2023). Representation Learning for Texts and Graphs: A Unified Perspective On Efficiency, Multimodality, and Adaptability. Number 2023/1 in Kiel Computer Science Series. Department of Computer Science, 2023. Dissertation, Faculty of Engineering, Kiel University. [pdf](https://doi.org/10.21941/kcss/2023/1)

## Project reports

- [r3] Iacopo Vagliano, Till Blume, Lukas Galke, Florian Mai, Ahmed Saleh, Alexandros Pournaras, Nikolaos Gkalelis, Damianos Galanopoulos, Vasileios Mezaris, Ilija Šimić, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Andrea Zielinski, Peter Mutschke (2019). Deliverable 3.3: Technologies for MOVING data processing and visualisation v3.0. [report](http://moving-project.eu/wp-content/uploads/2019/03/moving_d3.3_v1.0.pdf)
- [r2] Iacopo Vagliano, Mohammad Abdel-Qader, Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis, Ilija Šimić, Cecilia di Sciascio, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke (2018). Technologies for MOVING data processing and visualisation v2.0. [report](http://moving-project.eu/wp-content/uploads/2018/03/moving_d3.2_v1.0.pdf)
- [r1] Till Blume, Falk Böschen, Lukas Galke, Ahmed Saleh, Ansgar Scherp, Matthias Schulte-Althoff, Chrysa Collyda, Vasileios Mezaris, Alexandros Pournaras, Christos Tzelepis,  Peter Hasitschka, Vedran Sabol, Aitor Apaolaza, Markel Vigo, Tobias Backes, Peter Mutschke Thomas Gottron (2017). Technologies for MOVING data processing and visualisation v1.0. [report](http://moving-project.eu/wp-content/uploads/2017/04/moving_d3.1_v1.0.pdf)